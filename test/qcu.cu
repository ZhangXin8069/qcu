#include <cstdio>
#include <cstdlib>
#include <cublas_v2.h>
#include <cuda_runtime.h>
#include <curand_kernel.h>
#include <vector>
#include "cublas_utils.h"
#include "curand_utils.h"
#include "checker_utils.h"
#include "timer_utils.h"
using data_type = cuDoubleComplex;

/*
 * This example demonstrates two techniques for using the cuRAND host and device
 * API to generate random numbers for CUDA kernels to consume.
 */

int threads_per_block = 256;
int blocks_per_grid = 30;

/*
 * host_api_kernel consumes pre-generated random values from the cuRAND host API
 * to perform some dummy computation.
 */
__global__ void host_api_kernel(data_type *randomValues, data_type *out,
                                int N) {
  int i;
  int tid = blockIdx.x * blockDim.x + threadIdx.x;
  int nthreads = gridDim.x * blockDim.x;

  for (i = tid; i < N; i += nthreads) {
    data_type rand = randomValues[i];
    rand = rand * 2;
    out[i] = rand;
  }
}

/*
 * device_api_kernel uses the cuRAND device API to generate random numbers
 * on-the-fly on the GPU, and then performs some dummy computation using them.
 */
__global__ void device_api_kernel(curandState *states, data_type *out, int N) {
  int i;
  int tid = blockIdx.x * blockDim.x + threadIdx.x;
  int nthreads = gridDim.x * blockDim.x;
  curandState *state = states + tid;

  curand_init(9384, tid, 0, state);

  for (i = tid; i < N; i += nthreads) {
    data_type rand = curand_uniform(state);
    rand = rand * 2;
    out[i] = rand;
  }
}

/*
 * use_host_api is an examples usage of the cuRAND host API to generate random
 * values to be consumed on the device.
 */
void use_host_api(int N) {
  int i;
  curandGenerator_t randGen;
  data_type *dRand, *dOut, *hOut;

  // Create cuRAND generator (i.e. handle)
  CUDA_CHECK(curandCreateGenerator(&randGen, CURAND_RNG_PSEUDO_DEFAULT));

  // Allocate device memory to store the random values and output
  CUDA_CHECK(cudaMalloc((void **)&dRand, sizeof(data_type) * N));
  CUDA_CHECK(cudaMalloc((void **)&dOut, sizeof(data_type) * N));
  hOut = (data_type *)malloc(sizeof(data_type) * N);

  // Generate N random values from a uniform distribution
  CUDA_CHECK(curandGenerateUniform(randGen, dRand, N));

  // Consume the values generated by curandGenerateUniform
  host_api_kernel<<<blocks_per_grid, threads_per_block>>>(dRand, dOut, N);

  // Retrieve outputs
  CUDA_CHECK(cudaMemcpy(hOut, dOut, sizeof(data_type) * N, cudaMemcpyDeviceToHost));

  printf("Sampling of output from host API:\n");

  for (i = 0; i < 10; i++) {
    printf("%2.4f\n", hOut[i]);
  }

  printf("...\n");

  free(hOut);
  CUDA_CHECK(cudaFree(dRand));
  CUDA_CHECK(cudaFree(dOut));
  CUDA_CHECK(curandDestroyGenerator(randGen));
}

/*
 * use_device_api is an examples usage of the cuRAND device API to use the GPU
 * to generate random values on the fly from inside a CUDA kernel.
 */
void use_device_api(int N) {
  int i;
  static curandState *states = NULL;
  data_type *dOut, *hOut;

  /*
   * Allocate device memory to store the output and cuRAND device state
   * objects (which are analogous to handles, but on the GPU).
   */
  CUDA_CHECK(cudaMalloc((void **)&dOut, sizeof(data_type) * N));
  CUDA_CHECK(cudaMalloc((void **)&states,
                   sizeof(curandState) * threads_per_block * blocks_per_grid));
  hOut = (data_type *)malloc(sizeof(data_type) * N);

  // Execute a kernel that generates and consumes its own random numbers
  device_api_kernel<<<blocks_per_grid, threads_per_block>>>(states, dOut, N);

  // Retrieve the results
  CUDA_CHECK(cudaMemcpy(hOut, dOut, sizeof(data_type) * N, cudaMemcpyDeviceToHost));

  printf("Sampling of output from device API:\n");

  for (i = 0; i < 10; i++) {
    printf("%2.4f\n", hOut[i]);
  }

  printf("...\n");

  free(hOut);
  CUDA_CHECK(cudaFree(dOut));
  CUDA_CHECK(cudaFree(states));
}

int main(int argc, char *argv[]) {
  cublasHandle_t cublasH = NULL;
  cudaStream_t stream = NULL;

  /*
   *   A = | 1.1 + 1.2j | 2.3 + 2.4j | 3.5 + 3.6j | 4.7 + 4.8j |
   *   B = | 5.1 + 5.2j | 6.3 + 6.4j | 7.5 + 7.6j | 8.7 + 8.8j |
   */

  const std::vector<data_type> A = {
      {1.1, 1.2}, {2.3, 2.4}, {3.5, 3.6}, {4.7, 4.8}};
  std::vector<data_type> B = {{5.1, 5.2}, {6.3, 6.4}, {7.5, 7.6}, {8.7, 8.8}};
  const data_type alpha = {2.1, 1};
  const int incx = 1;
  const int incy = 1;

  data_type *d_A = nullptr;
  data_type *d_B = nullptr;

  printf("A\n");
  print_vector(A.size(), A.data());
  printf("=====\n");

  printf("B\n");
  print_vector(B.size(), B.data());
  printf("=====\n");

  /* step 1: create cublas handle, bind a stream */
  CUBLAS_CHECK(cublasCreate(&cublasH));

  CUDA_CHECK(cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking));
  CUBLAS_CHECK(cublasSetStream(cublasH, stream));

  /* step 2: copy data to device */
  CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_A),
                        sizeof(data_type) * A.size()));
  CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_B),
                        sizeof(data_type) * B.size()));

  CUDA_CHECK(cudaMemcpyAsync(d_A, A.data(), sizeof(data_type) * A.size(),
                             cudaMemcpyHostToDevice, stream));
  CUDA_CHECK(cudaMemcpyAsync(d_B, B.data(), sizeof(data_type) * B.size(),
                             cudaMemcpyHostToDevice, stream));

  /* step 3: compute */
  CUBLAS_CHECK(cublasAxpyEx(cublasH, A.size(), &alpha,
                            traits<data_type>::cuda_data_type, d_A,
                            traits<data_type>::cuda_data_type, incx, d_B,
                            traits<data_type>::cuda_data_type, incy,
                            traits<data_type>::cuda_data_type));

  /* step 4: copy data to host */
  CUDA_CHECK(cudaMemcpyAsync(B.data(), d_B, sizeof(data_type) * B.size(),
                             cudaMemcpyDeviceToHost, stream));

  CUDA_CHECK(cudaStreamSynchronize(stream));

  /*
   *   B = | 7.10 10.20 13.30 16.40 |
   */

  printf("B\n");
  print_vector(B.size(), B.data());
  printf("=====\n");

  /* free resources */
  CUDA_CHECK(cudaFree(d_A));
  CUDA_CHECK(cudaFree(d_B));

  CUBLAS_CHECK(cublasDestroy(cublasH));

  CUDA_CHECK(cudaStreamDestroy(stream));

  CUDA_CHECK(cudaDeviceReset());

  {

    int N = 8388608;

    use_host_api(N);
    use_device_api(N);
  }
  return EXIT_SUCCESS;
}

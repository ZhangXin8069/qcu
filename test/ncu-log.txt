==PROF== Connected to process 196950 (/usr/bin/python3.10)
Disabling GPU-Direct RDMA access
Disabling peer-to-peer access
QUDA 1.1.0 (git 1.1.0--sm_80)
CUDA Driver version = 12060
CUDA Runtime version = 12030
Found device 0: NVIDIA GeForce RTX 4060 Laptop GPU
WARNING: ** Running on a device with compute capability 8.9 but QUDA was compiled for 8.0. **
 -- This might result in a lower performance. Please consider adjusting QUDA_GPU_ARCH when running cmake.

Using device 0: NVIDIA GeForce RTX 4060 Laptop GPU
WARNING: Data reordering done on GPU (set with QUDA_REORDER_LOCATION=GPU/CPU)
WARNING: Autotuning disabled
WARNING: Autotuning disabled
WARNING: Using device memory pool allocator
WARNING: Using pinned memory pool allocator
cublasCreated successfully
==PROF== Profiling "generate_seed_pseudo" - 0: 0%....50%....100% - 9 passes
==PROF== Profiling "gen_sequenced" - 1: 0%....50%....100% - 9 passes
==PROF== Profiling "cupy_fill" - 2: 0%....50%....100% - 9 passes
==PROF== Profiling "cupy_copy__float64_complex128" - 3: 0%....50%....100% - 9 passes
==PROF== Profiling "Kernel3D" - 4: 0%....50%....100% - 9 passes
==PROF== Profiling "Kernel3D" - 5: 0%....50%....100% - 9 passes
==PROF== Profiling "Kernel3D" - 6: 0%....50%....100% - 9 passes
==PROF== Profiling "Kernel3D" - 7: 0%....50%....100% - 9 passes
==PROF== Profiling "Kernel3D" - 8: 0%....50%....100% - 9 passes
==PROF== Profiling "Kernel3D" - 9: 0%....50%....100% - 9 passes
==PROF== Profiling "Kernel3D" - 10: 0%....50%....100% - 9 passes
==PROF== Profiling "Kernel2D" - 11: 0%....50%....100% - 9 passes
Creating Gaussian distrbuted Lie group field with sigma = 1.000000e-01
==PROF== Profiling "Kernel2D" - 12: 0%....50%....100% - 9 passes
==PROF== Profiling "Kernel3D" - 13: 0%....50%....100% - 9 passes
==PROF== Profiling "Kernel3D" - 14: 0%....50%....100% - 9 passes
==PROF== Profiling "Kernel3D" - 15: 0%....50%....100% - 9 passes
==PROF== Profiling "cupy_copy__complex128_complex..." - 16: 0%....50%....100% - 9 passes
==PROF== Profiling "Kernel3D" - 17: 0%....50%....100% - 9 passes
==PROF== Profiling "Kernel3D" - 18: 0%....50%....100% - 9 passes
==PROF== Profiling "Kernel3D" - 19: 0%....50%....100% - 9 passes
==PROF== Profiling "Kernel3D" - 20: 0%....50%....100% - 9 passes
==PROF== Profiling "Kernel3D" - 21: 0%....50%....100% - 9 passes
==PROF== Profiling "Kernel3D" - 22: 0%....50%....100% - 9 passes
==PROF== Profiling "Kernel3D" - 23: 0%....50%....100% - 9 passes
==PROF== Profiling "Kernel2D" - 24: 0%....50%....100% - 9 passes
==PROF== Profiling "Kernel3D" - 25: 0%....50%....100% - 9 passes
==PROF== Profiling "Kernel2D" - 26: 0%....50%....100% - 9 passes
==PROF== Profiling "Kernel2D" - 27: 0%....50%....100% - 9 passes
==PROF== Profiling "Kernel3D" - 28: 0%....50%....100% - 9 passes
==PROF== Profiling "Kernel2D" - 29: 0%....50%....100% - 9 passes
==PROF== Profiling "_dptzyxcc2ccdptzyx" - 30: 0%
==WARNING== An error was reported by the driver

==WARNING== Backing up device memory in system memory. Kernel replay might be slow. Consider using "--replay-mode application" to avoid memory save-and-restore.
....50%....100% - 9 passes
==PROF== Profiling "copy_kernel" - 31: 0%....50%....100% - 9 passes
==PROF== Profiling "_tzyxsc2sctzyx" - 32: 0%....50%....100% - 9 passes
==PROF== Profiling "copy_kernel" - 33: 0%....50%....100% - 9 passes
==PROF== Profiling "_tzyxsc2sctzyx" - 34: 0%....50%....100% - 9 passes
==PROF== Profiling "copy_kernel" - 35: 0%....50%....100% - 9 passes
==PROF== Profiling "wilson_dslash_x_send" - 36: 0%....50%....100% - 9 passes
==PROF== Profiling "wilson_dslash_y_send" - 37: 0%....50%....100% - 9 passes
==PROF== Profiling "wilson_dslash_z_send" - 38: 0%....50%....100% - 9 passes
==PROF== Profiling "wilson_dslash_t_send" - 39: 0%....50%....100% - 9 passes
==PROF== Profiling "wilson_dslash_inside" - 40: 0%....50%....100% - 9 passes
==PROF== Profiling "wilson_dslash_x_recv" - 41: 0%....50%....100% - 9 passes
==PROF== Profiling "wilson_dslash_y_recv" - 42: 0%....50%....100% - 9 passes
==PROF== Profiling "wilson_dslash_z_recv" - 43: 0%....50%....100% - 9 passes
==PROF== Profiling "wilson_dslash_t_recv" - 44: 0%....50%....100% - 9 passes
==PROF== Profiling "_ccdptzyx2dptzyxcc" - 45: 0%....50%....100% - 9 passes
==PROF== Profiling "copy_kernel" - 46: 0%....50%....100% - 9 passes
==PROF== Profiling "_sctzyx2tzyxsc" - 47: 0%....50%....100% - 9 passes
==PROF== Profiling "copy_kernel" - 48: 0%....50%....100% - 9 passes
==PROF== Profiling "_sctzyx2tzyxsc" - 49: 0%....50%....100% - 9 passes
==PROF== Profiling "copy_kernel" - 50: 0%....50%....100% - 9 passes
==PROF== Profiling "_dptzyxcc2ccdptzyx" - 51: 0%....50%....100% - 9 passes
==PROF== Profiling "copy_kernel" - 52: 0%....50%....100% - 9 passes
==PROF== Profiling "_tzyxsc2sctzyx" - 53: 0%....50%....100% - 9 passes
==PROF== Profiling "copy_kernel" - 54: 0%....50%....100% - 9 passes
==PROF== Profiling "_tzyxsc2sctzyx" - 55: 0%....50%....100% - 9 passes
==PROF== Profiling "copy_kernel" - 56: 0%....50%....100% - 9 passes
==PROF== Profiling "wilson_dslash_x_send" - 57: 0%....50%....100% - 9 passes
==PROF== Profiling "wilson_dslash_y_send" - 58: 0%....50%....100% - 9 passes
==PROF== Profiling "wilson_dslash_z_send" - 59: 0%....50%....100% - 9 passes
==PROF== Profiling "wilson_dslash_t_send" - 60: 0%....50%....100% - 9 passes
==PROF== Profiling "wilson_dslash_inside" - 61: 0%....50%....100% - 9 passes
==PROF== Profiling "wilson_dslash_x_recv" - 62: 0%....50%....100% - 9 passes
==PROF== Profiling "wilson_dslash_y_recv" - 63: 0%....50%....100% - 9 passes
==PROF== Profiling "wilson_dslash_z_recv" - 64: 0%....50%....100% - 9 passes
==PROF== Profiling "wilson_dslash_t_recv" - 65: 0%....50%....100% - 9 passes
==PROF== Profiling "_ccdptzyx2dptzyxcc" - 66: 0%....50%....100% - 9 passes
==PROF== Profiling "copy_kernel" - 67: 0%....50%....100% - 9 passes
==PROF== Profiling "_sctzyx2tzyxsc" - 68: 0%....50%....100% - 9 passes
==PROF== Profiling "copy_kernel" - 69: 0%....50%....100% - 9 passes
==PROF== Profiling "_sctzyx2tzyxsc" - 70: 0%....50%....100% - 9 passes
==PROF== Profiling "copy_kernel" - 71: 0%....50%....100% - 9 passes
==PROF== Profiling "cupy_subtract__complex128_com..." - 72: 0%....50%....100% - 9 passes
==PROF== Profiling "cupy_absolute__complex128_flo..." - 73: 0%....50%....100% - 9 passes
==PROF== Profiling "cupy_multiply__float64_float6..." - 74: 0%....50%....100% - 9 passes
==PROF== Profiling "DeviceReduceKernel" - 75: 0%....50%....100% - 9 passes
==PROF== Profiling "DeviceReduceSingleTileKernel" - 76: 0%....50%....100% - 9 passes
==PROF== Profiling "cupy_sqrt__float64_float64" - 77: 0%....50%....100% - 9 passes
==PROF== Profiling "cupy_absolute__complex128_flo..." - 78: 0%....50%....100% - 9 passes
==PROF== Profiling "cupy_multiply__float64_float6..." - 79: 0%....50%....100% - 9 passes
==PROF== Profiling "DeviceReduceKernel" - 80: 0%....50%....100% - 9 passes
==PROF== Profiling "DeviceReduceSingleTileKernel" - 81: 0%....50%....100% - 9 passes
==PROF== Profiling "cupy_sqrt__float64_float64" - 82: 0%....50%....100% - 9 passes
==PROF== Profiling "cupy_true_divide__float64_flo..." - 83: 0%....50%....100% - 9 passes
===============round  0 ======================
Quda dslash: 9.169316562001768 sec
QCU dslash: 106.0193557629973 sec
quda difference:  2.805071587072014e-16
nccl wilson dslash total time: (without malloc free memcpy) :18.189737594 sec
nccl wilson dslash total time: (without malloc free memcpy) :18.833322312 sec

               initQuda Total time =     0.021 secs
                     init     =     0.021 secs ( 99.972%),	 with        2 calls at 1.060e+04 us per call
        total accounted       =     0.021 secs ( 99.972%)
        total missing         =     0.000 secs (  0.028%)

          loadGaugeQuda Total time =    20.246 secs
                 download     =     8.709 secs ( 43.016%),	 with        2 calls at 4.355e+06 us per call
                   upload     =     1.866 secs (  9.216%),	 with        1 calls at 1.866e+06 us per call
                     init     =     0.005 secs (  0.025%),	 with        2 calls at 2.504e+03 us per call
                  compute     =     9.666 secs ( 47.743%),	 with        2 calls at 4.833e+06 us per call
                     free     =     0.000 secs (  0.000%),	 with        2 calls at 2.500e+01 us per call
        total accounted       =    20.246 secs (100.000%)
        total missing         =     0.000 secs (  0.000%)

             dslashQuda Total time =     9.169 secs
                 download     =     2.849 secs ( 31.076%),	 with        2 calls at 1.425e+06 us per call
                   upload     =     2.670 secs ( 29.115%),	 with        2 calls at 1.335e+06 us per call
                     init     =     0.001 secs (  0.007%),	 with        2 calls at 3.055e+02 us per call
                  compute     =     3.649 secs ( 39.801%),	 with        2 calls at 1.825e+06 us per call
                     free     =     0.000 secs (  0.000%),	 with        2 calls at 6.500e+00 us per call
        total accounted       =     9.169 secs (100.000%)
        total missing         =     0.000 secs (  0.000%)

                endQuda Total time =     0.022 secs

       initQuda-endQuda Total time =   172.486 secs

                   QUDA Total time =    40.454 secs
                 download     =    11.559 secs ( 28.572%),	 with        4 calls at 2.890e+06 us per call
                   upload     =     4.535 secs ( 11.211%),	 with        3 calls at 1.512e+06 us per call
                     init     =     0.027 secs (  0.066%),	 with        6 calls at 4.470e+03 us per call
                  compute     =    24.312 secs ( 60.096%),	 with        5 calls at 4.862e+06 us per call
                     free     =     0.000 secs (  0.000%),	 with        4 calls at 1.525e+01 us per call
        total accounted       =    40.432 secs ( 99.946%)
        total missing         =     0.022 secs (  0.054%)

Device memory used = 1380.0 MiB
Pinned device memory used = 0.0 MiB
Managed memory used = 0.0 MiB
Shmem memory used = 0.0 MiB
Page-locked host memory used = 48.0 MiB
Total host memory used >= 66.0 MiB

==PROF== Disconnected from process 196950
[196950] python3.10@127.0.0.1
  void generate_seed_pseudo<rng_config<curandStateXORWOW, (curandOrdering)101>>(unsigned long long, unsigned long long, unsigned long long, curandOrdering, curandStateXORWOW *, unsigned int *) (64, 1, 1)x(64, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/usecond       783.76
    SM Frequency            cycle/usecond       367.35
    Elapsed Cycles                  cycle       301652
    Memory Throughput                   %        35.83
    DRAM Throughput                     %         1.26
    Duration                      usecond       821.15
    L1/TEX Cache Throughput             %        39.35
    L2 Cache Throughput                 %         0.95
    SM Active Cycles                cycle    274711.25
    Compute (SM) Throughput             %        35.83
    ----------------------- ------------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     64
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread            4096
    Waves Per SM                                                0.11
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           24
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           24
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        10.54
    Achieved Active Warps Per SM           warp         5.06
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 89.46%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (10.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

  void gen_sequenced<curandStateXORWOW, double2, normal_args_double_st, &curand_normal_scaled2_double<curandStateXORWOW>, rng_config<curandStateXORWOW, (curandOrdering)101>>(T1 *, T2 *, unsigned long, unsigned long, T3) (64, 1, 1)x(64, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/usecond       231.63
    SM Frequency            cycle/usecond        98.38
    Elapsed Cycles                  cycle     24871034
    Memory Throughput                   %        10.02
    DRAM Throughput                     %        10.02
    Duration                      msecond       252.80
    L1/TEX Cache Throughput             %         2.11
    L2 Cache Throughput                 %         0.42
    SM Active Cycles                cycle  21989052.42
    Compute (SM) Throughput             %        75.89
    ----------------------- ------------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     64
    Registers Per Thread             register/thread              42
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread            4096
    Waves Per SM                                                0.13
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           20
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           24
    Theoretical Active Warps per SM        warp           40
    Theoretical Occupancy                     %        83.33
    Achieved Occupancy                        %        10.62
    Achieved Active Warps Per SM           warp         5.10
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 87.25%                                                                                    
          The difference between calculated theoretical (83.3%) and measured achieved occupancy (10.6%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

  cupy_fill (1, 1, 1)x(3, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         7.53
    SM Frequency            cycle/usecond        49.49
    Elapsed Cycles                  cycle         1316
    Memory Throughput                   %         0.68
    DRAM Throughput                     %         0.00
    Duration                      usecond        26.59
    L1/TEX Cache Throughput             %        37.70
    L2 Cache Throughput                 %         0.06
    SM Active Cycles                cycle        23.88
    Compute (SM) Throughput             %         0.03
    ----------------------- ------------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     3
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread               3
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 90.62%                                                                                          
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 3      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 95.83%                                                                                          
          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 24              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block          128
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %         2.08
    Achieved Active Warps Per SM           warp            1
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 95.83%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (2.1%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

  cupy_copy__float64_complex128 (294912, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/usecond       800.43
    SM Frequency            cycle/usecond       293.42
    Elapsed Cycles                  cycle      8089330
    Memory Throughput                   %        82.35
    DRAM Throughput                     %        82.35
    Duration                      msecond        27.57
    L1/TEX Cache Throughput             %        19.45
    L2 Cache Throughput                 %        14.22
    SM Active Cycles                cycle   7928230.33
    Compute (SM) Throughput             %         8.96
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 294912
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread        37748736
    Waves Per SM                                                1024
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           32
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        84.02
    Achieved Active Warps Per SM           warp        40.33
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 15.98%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (84.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

  std::enable_if<device::use_kernel_arg<T2>(), void>::type quda::Kernel3D<quda::CopyGauge_, quda::CopyGaugeArg<double, double, (int)18, (bool)0, quda::gauge::FloatNOrder<double, (int)18, (int)2, (int)12, (QudaStaggeredPhase_s)0, (bool)1, (QudaGhostExchange_s)-2147483648, (bool)0>, quda::gauge::QDPOrder<double, (int)18>>, (bool)0>(T2) (16384, 1, 8)x(32, 1, 1), Context 1, Stream 21, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/usecond       807.07
    SM Frequency            cycle/usecond       283.97
    Elapsed Cycles                  cycle     13688750
    Memory Throughput                   %        79.83
    DRAM Throughput                     %        79.83
    Duration                      msecond        48.21
    L1/TEX Cache Throughput             %         9.57
    L2 Cache Throughput                 %         2.18
    SM Active Cycles                cycle  13333622.96
    Compute (SM) Throughput             %         4.79
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 131072
    Registers Per Thread             register/thread              48
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread         4194304
    Waves Per SM                                              227.56
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           40
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        49.82
    Achieved Active Warps Per SM           warp        23.92
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

  std::enable_if<device::use_kernel_arg<T2>(), void>::type quda::Kernel3D<quda::GhostExtractor, quda::ExtractGhostArg<double, (int)3, quda::gauge::FloatNOrder<double, (int)18, (int)2, (int)12, (QudaStaggeredPhase_s)0, (bool)1, (QudaGhostExchange_s)-2147483648, (bool)0>, (bool)1>, (bool)0>(T2) (1024, 1, 8)x(32, 1, 1), Context 1, Stream 21, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/usecond       770.45
    SM Frequency            cycle/usecond       318.17
    Elapsed Cycles                  cycle       288614
    Memory Throughput                   %        77.68
    DRAM Throughput                     %        77.68
    Duration                      usecond       907.10
    L1/TEX Cache Throughput             %        16.33
    L2 Cache Throughput                 %         3.99
    SM Active Cycles                cycle    270933.92
    Compute (SM) Throughput             %         5.22
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   8192
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread          262144
    Waves Per SM                                               14.22
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           48
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        48.24
    Achieved Active Warps Per SM           warp        23.15
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

  std::enable_if<device::use_kernel_arg<T2>(), void>::type quda::Kernel3D<quda::CopyGhost_, quda::CopyGaugeArg<double, double, (int)18, (bool)0, quda::gauge::FloatNOrder<double, (int)18, (int)2, (int)12, (QudaStaggeredPhase_s)0, (bool)1, (QudaGhostExchange_s)-2147483648, (bool)0>, quda::gauge::FloatNOrder<double, (int)18, (int)2, (int)12, (QudaStaggeredPhase_s)0, (bool)1, (QudaGhostExchange_s)-2147483648, (bool)0>>, (bool)0>(T2) (512, 1, 8)x(32, 1, 1), Context 1, Stream 21, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/usecond       783.38
    SM Frequency            cycle/usecond       295.42
    Elapsed Cycles                  cycle       193866
    Memory Throughput                   %        79.70
    DRAM Throughput                     %        79.70
    Duration                      usecond       656.19
    L1/TEX Cache Throughput             %        19.29
    L2 Cache Throughput                 %         3.67
    SM Active Cycles                cycle    184204.75
    Compute (SM) Throughput             %         3.70
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread          131072
    Waves Per SM                                                7.11
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           48
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        47.27
    Achieved Active Warps Per SM           warp        22.69
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

  std::enable_if<device::use_kernel_arg<T2>(), void>::type quda::Kernel3D<quda::CopyGauge_, quda::CopyGaugeArg<short, double, (int)18, (bool)0, quda::gauge::FloatNOrder<short, (int)18, (int)4, (int)12, (QudaStaggeredPhase_s)0, (bool)1, (QudaGhostExchange_s)-2147483648, (bool)0>, quda::gauge::FloatNOrder<double, (int)18, (int)2, (int)12, (QudaStaggeredPhase_s)0, (bool)1, (QudaGhostExchange_s)-2147483648, (bool)0>>, (bool)0>(T2) (16384, 1, 8)x(32, 1, 1), Context 1, Stream 21, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/usecond       805.58
    SM Frequency            cycle/nsecond         1.91
    Elapsed Cycles                  cycle     47956503
    Memory Throughput                   %        77.00
    DRAM Throughput                     %        77.00
    Duration                      msecond        25.05
    L1/TEX Cache Throughput             %         1.06
    L2 Cache Throughput                 %         1.74
    SM Active Cycles                cycle  49467464.67
    Compute (SM) Throughput             %         4.37
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 131072
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread         4194304
    Waves Per SM                                              227.56
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           64
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        49.84
    Achieved Active Warps Per SM           warp        23.92
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

  std::enable_if<device::use_kernel_arg<T2>(), void>::type quda::Kernel3D<quda::CopyGhost_, quda::CopyGaugeArg<short, double, (int)18, (bool)0, quda::gauge::FloatNOrder<short, (int)18, (int)4, (int)12, (QudaStaggeredPhase_s)0, (bool)1, (QudaGhostExchange_s)-2147483648, (bool)0>, quda::gauge::FloatNOrder<double, (int)18, (int)2, (int)12, (QudaStaggeredPhase_s)0, (bool)1, (QudaGhostExchange_s)-2147483648, (bool)0>>, (bool)0>(T2) (512, 1, 8)x(32, 1, 1), Context 1, Stream 21, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         1.95
    SM Frequency            cycle/nsecond         3.29
    Elapsed Cycles                  cycle       817548
    Memory Throughput                   %        81.18
    DRAM Throughput                     %        81.18
    Duration                      usecond       248.19
    L1/TEX Cache Throughput             %         2.81
    L2 Cache Throughput                 %         3.01
    SM Active Cycles                cycle    582093.96
    Compute (SM) Throughput             %         8.02
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread          131072
    Waves Per SM                                                7.11
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           64
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        48.07
    Achieved Active Warps Per SM           warp        23.07
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

  std::enable_if<device::use_kernel_arg<T2>(), void>::type quda::Kernel3D<quda::CopyGauge_, quda::CopyGaugeArg<float, double, (int)18, (bool)0, quda::gauge::FloatNOrder<float, (int)18, (int)4, (int)12, (QudaStaggeredPhase_s)0, (bool)1, (QudaGhostExchange_s)-2147483648, (bool)0>, quda::gauge::FloatNOrder<double, (int)18, (int)2, (int)12, (QudaStaggeredPhase_s)0, (bool)1, (QudaGhostExchange_s)-2147483648, (bool)0>>, (bool)0>(T2) (16384, 1, 8)x(32, 1, 1), Context 1, Stream 21, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.00
    SM Frequency            cycle/usecond       916.34
    Elapsed Cycles                  cycle      2280002
    Memory Throughput                   %        92.85
    DRAM Throughput                     %        92.85
    Duration                      msecond         2.49
    L1/TEX Cache Throughput             %        28.80
    L2 Cache Throughput                 %        37.48
    SM Active Cycles                cycle   4351505.96
    Compute (SM) Throughput             %        91.98
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 131072
    Registers Per Thread             register/thread              31
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread         4194304
    Waves Per SM                                              227.56
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           64
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        49.35
    Achieved Active Warps Per SM           warp        23.69
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

  std::enable_if<device::use_kernel_arg<T2>(), void>::type quda::Kernel3D<quda::CopyGhost_, quda::CopyGaugeArg<float, double, (int)18, (bool)0, quda::gauge::FloatNOrder<float, (int)18, (int)4, (int)12, (QudaStaggeredPhase_s)0, (bool)1, (QudaGhostExchange_s)-2147483648, (bool)0>, quda::gauge::FloatNOrder<double, (int)18, (int)2, (int)12, (QudaStaggeredPhase_s)0, (bool)1, (QudaGhostExchange_s)-2147483648, (bool)0>>, (bool)0>(T2) (512, 1, 8)x(32, 1, 1), Context 1, Stream 21, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/usecond       804.57
    SM Frequency            cycle/nsecond         1.91
    Elapsed Cycles                  cycle      1147062
    Memory Throughput                   %        81.53
    DRAM Throughput                     %        81.53
    Duration                      usecond       599.78
    L1/TEX Cache Throughput             %         1.79
    L2 Cache Throughput                 %         2.27
    SM Active Cycles                cycle   1212964.29
    Compute (SM) Throughput             %         5.71
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread          131072
    Waves Per SM                                                7.11
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           64
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        48.45
    Achieved Active Warps Per SM           warp        23.26
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

  std::enable_if<device::use_kernel_arg<T2>(), void>::type quda::Kernel2D<quda::init_random, quda::rngArg, (bool)0>(T2) (16384, 2, 1)x(32, 1, 1), Context 1, Stream 21, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- -------------
    Metric Name               Metric Unit  Metric Value
    ----------------------- ------------- -------------
    DRAM Frequency          cycle/nsecond          8.82
    SM Frequency            cycle/nsecond          1.90
    Elapsed Cycles                  cycle    1112865271
    Memory Throughput                   %          0.49
    DRAM Throughput                     %          0.05
    Duration                      msecond        586.67
    L1/TEX Cache Throughput             %          0.60
    L2 Cache Throughput                 %          0.49
    SM Active Cycles                cycle 1109913905.50
    Compute (SM) Throughput             %         86.61
    ----------------------- ------------- -------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  32768
    Registers Per Thread             register/thread              86
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread         1048576
    Waves Per SM                                               68.27
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           20
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           20
    Theoretical Occupancy                     %        41.67
    Achieved Occupancy                        %        36.96
    Achieved Active Warps Per SM           warp        17.74
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 58.33%                                                                                    
          The 5.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (41.7%) is limited by the number of required      
          registers.                                                                                                    

  std::enable_if<device::use_kernel_arg<T2>(), void>::type quda::Kernel2D<quda::GaussGauge, quda::GaugeGaussArg<double, (int)3, (QudaReconstructType_s)12, (bool)1>, (bool)0>(T2) (16384, 2, 1)x(32, 1, 1), Context 1, Stream 21, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.42
    SM Frequency            cycle/nsecond         2.01
    Elapsed Cycles                  cycle    147957261
    Memory Throughput                   %         2.00
    DRAM Throughput                     %         2.00
    Duration                      msecond        73.55
    L1/TEX Cache Throughput             %         1.49
    L2 Cache Throughput                 %         1.14
    SM Active Cycles                cycle 147684943.42
    Compute (SM) Throughput             %        87.37
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  32768
    Registers Per Thread             register/thread             148
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread         1048576
    Waves Per SM                                              113.78
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           12
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        24.94
    Achieved Active Warps Per SM           warp        11.97
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75%                                                                                       
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (25.0%) is limited by the number of required      
          registers.                                                                                                    

  std::enable_if<device::use_kernel_arg<T2>(), void>::type quda::Kernel3D<quda::GhostExtractor, quda::ExtractGhostArg<double, (int)3, quda::gauge::FloatNOrder<double, (int)18, (int)2, (int)12, (QudaStaggeredPhase_s)0, (bool)1, (QudaGhostExchange_s)-2147483648, (bool)0>, (bool)1>, (bool)0>(T2) (1024, 1, 8)x(32, 1, 1), Context 1, Stream 21, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/usecond       767.49
    SM Frequency            cycle/nsecond         2.08
    Elapsed Cycles                  cycle      1860837
    Memory Throughput                   %        78.79
    DRAM Throughput                     %        78.79
    Duration                      usecond       894.59
    L1/TEX Cache Throughput             %         2.54
    L2 Cache Throughput                 %         2.77
    SM Active Cycles                cycle   1926214.12
    Compute (SM) Throughput             %         0.81
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   8192
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread          262144
    Waves Per SM                                               14.22
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           48
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        48.28
    Achieved Active Warps Per SM           warp        23.17
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

  std::enable_if<device::use_kernel_arg<T2>(), void>::type quda::Kernel3D<quda::CopyGhost_, quda::CopyGaugeArg<double, double, (int)18, (bool)0, quda::gauge::FloatNOrder<double, (int)18, (int)2, (int)12, (QudaStaggeredPhase_s)0, (bool)1, (QudaGhostExchange_s)-2147483648, (bool)0>, quda::gauge::FloatNOrder<double, (int)18, (int)2, (int)12, (QudaStaggeredPhase_s)0, (bool)1, (QudaGhostExchange_s)-2147483648, (bool)0>>, (bool)0>(T2) (512, 1, 8)x(32, 1, 1), Context 1, Stream 21, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/usecond       781.37
    SM Frequency            cycle/nsecond         1.76
    Elapsed Cycles                  cycle      1124105
    Memory Throughput                   %        81.19
    DRAM Throughput                     %        81.19
    Duration                      usecond       640.19
    L1/TEX Cache Throughput             %         3.38
    L2 Cache Throughput                 %         2.63
    SM Active Cycles                cycle   1106708.38
    Compute (SM) Throughput             %         0.64
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread          131072
    Waves Per SM                                                7.11
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           48
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        47.64
    Achieved Active Warps Per SM           warp        22.87
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

  std::enable_if<device::use_kernel_arg<T2>(), void>::type quda::Kernel3D<quda::CopyGauge_, quda::CopyGaugeArg<double, double, (int)18, (bool)0, quda::gauge::QDPOrder<double, (int)18>, quda::gauge::FloatNOrder<double, (int)18, (int)2, (int)12, (QudaStaggeredPhase_s)0, (bool)1, (QudaGhostExchange_s)-2147483648, (bool)0>>, (bool)0>(T2) (16384, 1, 8)x(32, 1, 1), Context 1, Stream 21, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/usecond       807.68
    SM Frequency            cycle/nsecond         1.87
    Elapsed Cycles                  cycle     90693367
    Memory Throughput                   %        78.88
    DRAM Throughput                     %        78.88
    Duration                      msecond        48.53
    L1/TEX Cache Throughput             %         1.89
    L2 Cache Throughput                 %        15.99
    SM Active Cycles                cycle  89947108.29
    Compute (SM) Throughput             %         3.18
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 131072
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread         4194304
    Waves Per SM                                              227.56
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           36
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        49.94
    Achieved Active Warps Per SM           warp        23.97
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

  cupy_copy__complex128_complex128 (294912, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/usecond       719.20
    SM Frequency            cycle/usecond       169.92
    Elapsed Cycles                  cycle      9856679
    Memory Throughput                   %        89.18
    DRAM Throughput                     %        89.18
    Duration                      msecond        58.00
    L1/TEX Cache Throughput             %        18.26
    L2 Cache Throughput                 %        14.63
    SM Active Cycles                cycle 107766927.33
    Compute (SM) Throughput             %         4.49
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 294912
    Registers Per Thread             register/thread              20
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread        37748736
    Waves Per SM                                                1024
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           21
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        92.43
    Achieved Active Warps Per SM           warp        44.37
    ------------------------------- ----------- ------------

  std::enable_if<device::use_kernel_arg<T2>(), void>::type quda::Kernel3D<quda::CopyGauge_, quda::CopyGaugeArg<double, double, (int)18, (bool)0, quda::gauge::FloatNOrder<double, (int)18, (int)2, (int)12, (QudaStaggeredPhase_s)0, (bool)1, (QudaGhostExchange_s)-2147483648, (bool)0>, quda::gauge::QDPOrder<double, (int)18>>, (bool)0>(T2) (16384, 1, 8)x(32, 1, 1), Context 1, Stream 21, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/usecond       807.16
    SM Frequency            cycle/nsecond         1.84
    Elapsed Cycles                  cycle     88401243
    Memory Throughput                   %        79.95
    DRAM Throughput                     %        79.95
    Duration                      msecond        48.12
    L1/TEX Cache Throughput             %         1.48
    L2 Cache Throughput                 %         1.41
    SM Active Cycles                cycle  87476322.92
    Compute (SM) Throughput             %         0.74
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 131072
    Registers Per Thread             register/thread              48
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread         4194304
    Waves Per SM                                              227.56
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           40
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        49.91
    Achieved Active Warps Per SM           warp        23.96
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

  std::enable_if<device::use_kernel_arg<T2>(), void>::type quda::Kernel3D<quda::GhostExtractor, quda::ExtractGhostArg<double, (int)3, quda::gauge::FloatNOrder<double, (int)18, (int)2, (int)12, (QudaStaggeredPhase_s)0, (bool)1, (QudaGhostExchange_s)-2147483648, (bool)0>, (bool)1>, (bool)0>(T2) (1024, 1, 8)x(32, 1, 1), Context 1, Stream 21, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/usecond       791.54
    SM Frequency            cycle/nsecond         2.25
    Elapsed Cycles                  cycle      1937904
    Memory Throughput                   %        79.06
    DRAM Throughput                     %        79.06
    Duration                      usecond       861.92
    L1/TEX Cache Throughput             %         2.44
    L2 Cache Throughput                 %         2.71
    SM Active Cycles                cycle   1928089.79
    Compute (SM) Throughput             %         0.78
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   8192
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread          262144
    Waves Per SM                                               14.22
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           48
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        48.54
    Achieved Active Warps Per SM           warp        23.30
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

  std::enable_if<device::use_kernel_arg<T2>(), void>::type quda::Kernel3D<quda::CopyGhost_, quda::CopyGaugeArg<double, double, (int)18, (bool)0, quda::gauge::FloatNOrder<double, (int)18, (int)2, (int)12, (QudaStaggeredPhase_s)0, (bool)1, (QudaGhostExchange_s)-2147483648, (bool)0>, quda::gauge::FloatNOrder<double, (int)18, (int)2, (int)12, (QudaStaggeredPhase_s)0, (bool)1, (QudaGhostExchange_s)-2147483648, (bool)0>>, (bool)0>(T2) (512, 1, 8)x(32, 1, 1), Context 1, Stream 21, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         7.95
    SM Frequency            cycle/usecond        26.10
    Elapsed Cycles                  cycle        39921
    Memory Throughput                   %        46.83
    DRAM Throughput                     %         3.37
    Duration                      msecond         1.53
    L1/TEX Cache Throughput             %        93.66
    L2 Cache Throughput                 %         1.66
    SM Active Cycles                cycle     38777.92
    Compute (SM) Throughput             %        17.96
    ----------------------- ------------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread          131072
    Waves Per SM                                                7.11
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           48
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        46.23
    Achieved Active Warps Per SM           warp        22.19
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

  std::enable_if<device::use_kernel_arg<T2>(), void>::type quda::Kernel3D<quda::CopyGauge_, quda::CopyGaugeArg<short, double, (int)18, (bool)0, quda::gauge::FloatNOrder<short, (int)18, (int)4, (int)12, (QudaStaggeredPhase_s)0, (bool)1, (QudaGhostExchange_s)-2147483648, (bool)0>, quda::gauge::FloatNOrder<double, (int)18, (int)2, (int)12, (QudaStaggeredPhase_s)0, (bool)1, (QudaGhostExchange_s)-2147483648, (bool)0>>, (bool)0>(T2) (16384, 1, 8)x(32, 1, 1), Context 1, Stream 21, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        24.43
    SM Frequency            cycle/usecond       284.55
    Elapsed Cycles                  cycle      2274847
    Memory Throughput                   %        23.05
    DRAM Throughput                     %         7.96
    Duration                      msecond         7.99
    L1/TEX Cache Throughput             %        23.07
    L2 Cache Throughput                 %         2.86
    SM Active Cycles                cycle   2272729.62
    Compute (SM) Throughput             %        92.19
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 131072
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread         4194304
    Waves Per SM                                              227.56
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           64
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        49.31
    Achieved Active Warps Per SM           warp        23.67
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

  std::enable_if<device::use_kernel_arg<T2>(), void>::type quda::Kernel3D<quda::CopyGhost_, quda::CopyGaugeArg<short, double, (int)18, (bool)0, quda::gauge::FloatNOrder<short, (int)18, (int)4, (int)12, (QudaStaggeredPhase_s)0, (bool)1, (QudaGhostExchange_s)-2147483648, (bool)0>, quda::gauge::FloatNOrder<double, (int)18, (int)2, (int)12, (QudaStaggeredPhase_s)0, (bool)1, (QudaGhostExchange_s)-2147483648, (bool)0>>, (bool)0>(T2) (512, 1, 8)x(32, 1, 1), Context 1, Stream 21, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/usecond       805.60
    SM Frequency            cycle/nsecond         1.99
    Elapsed Cycles                  cycle      1192025
    Memory Throughput                   %        81.44
    DRAM Throughput                     %        81.44
    Duration                      usecond       599.33
    L1/TEX Cache Throughput             %         1.31
    L2 Cache Throughput                 %         2.21
    SM Active Cycles                cycle   1246990.08
    Compute (SM) Throughput             %         5.50
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread          131072
    Waves Per SM                                                7.11
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           64
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        48.10
    Achieved Active Warps Per SM           warp        23.09
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

  std::enable_if<device::use_kernel_arg<T2>(), void>::type quda::Kernel3D<quda::CopyGauge_, quda::CopyGaugeArg<float, double, (int)18, (bool)0, quda::gauge::FloatNOrder<float, (int)18, (int)4, (int)12, (QudaStaggeredPhase_s)0, (bool)1, (QudaGhostExchange_s)-2147483648, (bool)0>, quda::gauge::FloatNOrder<double, (int)18, (int)2, (int)12, (QudaStaggeredPhase_s)0, (bool)1, (QudaGhostExchange_s)-2147483648, (bool)0>>, (bool)0>(T2) (16384, 1, 8)x(32, 1, 1), Context 1, Stream 21, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/usecond       807.45
    SM Frequency            cycle/nsecond         1.89
    Elapsed Cycles                  cycle     54642106
    Memory Throughput                   %        79.50
    DRAM Throughput                     %        79.50
    Duration                      msecond        28.89
    L1/TEX Cache Throughput             %         1.27
    L2 Cache Throughput                 %         1.52
    SM Active Cycles                cycle  53690530.58
    Compute (SM) Throughput             %         3.84
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 131072
    Registers Per Thread             register/thread              31
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread         4194304
    Waves Per SM                                              227.56
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           64
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        49.89
    Achieved Active Warps Per SM           warp        23.95
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

  std::enable_if<device::use_kernel_arg<T2>(), void>::type quda::Kernel3D<quda::CopyGhost_, quda::CopyGaugeArg<float, double, (int)18, (bool)0, quda::gauge::FloatNOrder<float, (int)18, (int)4, (int)12, (QudaStaggeredPhase_s)0, (bool)1, (QudaGhostExchange_s)-2147483648, (bool)0>, quda::gauge::FloatNOrder<double, (int)18, (int)2, (int)12, (QudaStaggeredPhase_s)0, (bool)1, (QudaGhostExchange_s)-2147483648, (bool)0>>, (bool)0>(T2) (512, 1, 8)x(32, 1, 1), Context 1, Stream 21, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/usecond       804.70
    SM Frequency            cycle/nsecond         1.94
    Elapsed Cycles                  cycle      1164574
    Memory Throughput                   %        81.49
    DRAM Throughput                     %        81.49
    Duration                      usecond       599.68
    L1/TEX Cache Throughput             %         1.76
    L2 Cache Throughput                 %         2.25
    SM Active Cycles                cycle   1177183.92
    Compute (SM) Throughput             %         5.63
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread          131072
    Waves Per SM                                                7.11
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           64
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        48.49
    Achieved Active Warps Per SM           warp        23.27
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

  std::enable_if<device::use_kernel_arg<T2>(), void>::type quda::Kernel2D<quda::CopyColorSpinor_, quda::CopyColorSpinorArg<double, double, (int)4, (int)3, quda::colorspinor::FloatNOrder<double, (int)4, (int)3, (int)2, (bool)0, (bool)0>, quda::colorspinor::SpaceSpinorColorOrder<double, (int)4, (int)3>, quda::NonRelBasis>, (bool)0>(T2) (16384, 1, 1)x(32, 1, 1), Context 1, Stream 21, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/usecond       808.35
    SM Frequency            cycle/nsecond         1.91
    Elapsed Cycles                  cycle     16944831
    Memory Throughput                   %        80.01
    DRAM Throughput                     %        80.01
    Duration                      msecond         8.89
    L1/TEX Cache Throughput             %         1.79
    L2 Cache Throughput                 %         1.41
    SM Active Cycles                cycle  17335788.96
    Compute (SM) Throughput             %         2.71
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  16384
    Registers Per Thread             register/thread              64
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread          524288
    Waves Per SM                                               28.44
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           32
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        49.61
    Achieved Active Warps Per SM           warp        23.81
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

  std::enable_if<device::use_kernel_arg<T2>(), void>::type quda::Kernel3D<quda::dslash_functor, quda::dslash_functor_arg<quda::wilson, quda::packShmem, (int)1, (bool)0, (bool)0, (quda::KernelType)5, quda::WilsonArg<double, (int)3, (int)4, (QudaReconstructType_s)12>>, (bool)0>(T2) (32768, 1, 1)x(16, 1, 1), Context 1, Stream 21, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/usecond       807.60
    SM Frequency            cycle/nsecond         1.76
    Elapsed Cycles                  cycle     54523306
    Memory Throughput                   %        75.45
    DRAM Throughput                     %        75.45
    Duration                      msecond        30.98
    L1/TEX Cache Throughput             %         2.65
    L2 Cache Throughput                 %         2.22
    SM Active Cycles                cycle  53802141.71
    Compute (SM) Throughput             %        48.08
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    16
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  32768
    Registers Per Thread             register/thread             130
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread          524288
    Waves Per SM                                              113.78
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           12
    Block Limit Shared Mem                block          100
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        24.24
    Achieved Active Warps Per SM           warp        11.64
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75%                                                                                       
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (25.0%) is limited by the number of required      
          registers.                                                                                                    

  std::enable_if<device::use_kernel_arg<T2>(), void>::type quda::Kernel2D<quda::CopyColorSpinor_, quda::CopyColorSpinorArg<double, double, (int)4, (int)3, quda::colorspinor::SpaceSpinorColorOrder<double, (int)4, (int)3>, quda::colorspinor::FloatNOrder<double, (int)4, (int)3, (int)2, (bool)0, (bool)0>, quda::RelBasis>, (bool)0>(T2) (16384, 1, 1)x(32, 1, 1), Context 1, Stream 21, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/usecond       804.38
    SM Frequency            cycle/nsecond         1.94
    Elapsed Cycles                  cycle     17320092
    Memory Throughput                   %        79.99
    DRAM Throughput                     %        79.99
    Duration                      msecond         8.92
    L1/TEX Cache Throughput             %         1.74
    L2 Cache Throughput                 %         1.39
    SM Active Cycles                cycle  17484244.08
    Compute (SM) Throughput             %         2.65
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  16384
    Registers Per Thread             register/thread              70
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread          524288
    Waves Per SM                                               28.44
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           28
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        49.65
    Achieved Active Warps Per SM           warp        23.83
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

  std::enable_if<device::use_kernel_arg<T2>(), void>::type quda::Kernel2D<quda::CopyColorSpinor_, quda::CopyColorSpinorArg<double, double, (int)4, (int)3, quda::colorspinor::FloatNOrder<double, (int)4, (int)3, (int)2, (bool)0, (bool)0>, quda::colorspinor::SpaceSpinorColorOrder<double, (int)4, (int)3>, quda::NonRelBasis>, (bool)0>(T2) (16384, 1, 1)x(32, 1, 1), Context 1, Stream 21, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/usecond       809.81
    SM Frequency            cycle/nsecond         1.89
    Elapsed Cycles                  cycle     16833373
    Memory Throughput                   %        79.98
    DRAM Throughput                     %        79.98
    Duration                      msecond         8.89
    L1/TEX Cache Throughput             %         1.80
    L2 Cache Throughput                 %         1.41
    SM Active Cycles                cycle  17353674.21
    Compute (SM) Throughput             %         2.73
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  16384
    Registers Per Thread             register/thread              64
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread          524288
    Waves Per SM                                               28.44
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           32
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        49.59
    Achieved Active Warps Per SM           warp        23.80
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

  std::enable_if<device::use_kernel_arg<T2>(), void>::type quda::Kernel3D<quda::dslash_functor, quda::dslash_functor_arg<quda::wilson, quda::packShmem, (int)1, (bool)0, (bool)0, (quda::KernelType)5, quda::WilsonArg<double, (int)3, (int)4, (QudaReconstructType_s)12>>, (bool)0>(T2) (32768, 1, 1)x(16, 1, 1), Context 1, Stream 21, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/usecond       807.50
    SM Frequency            cycle/nsecond         1.77
    Elapsed Cycles                  cycle     55076987
    Memory Throughput                   %        75.32
    DRAM Throughput                     %        75.32
    Duration                      msecond        31.03
    L1/TEX Cache Throughput             %         2.63
    L2 Cache Throughput                 %         2.22
    SM Active Cycles                cycle  54321909.92
    Compute (SM) Throughput             %        47.60
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    16
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  32768
    Registers Per Thread             register/thread             130
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread          524288
    Waves Per SM                                              113.78
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           12
    Block Limit Shared Mem                block          100
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        24.46
    Achieved Active Warps Per SM           warp        11.74
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75%                                                                                       
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (25.0%) is limited by the number of required      
          registers.                                                                                                    

  std::enable_if<device::use_kernel_arg<T2>(), void>::type quda::Kernel2D<quda::CopyColorSpinor_, quda::CopyColorSpinorArg<double, double, (int)4, (int)3, quda::colorspinor::SpaceSpinorColorOrder<double, (int)4, (int)3>, quda::colorspinor::FloatNOrder<double, (int)4, (int)3, (int)2, (bool)0, (bool)0>, quda::RelBasis>, (bool)0>(T2) (16384, 1, 1)x(32, 1, 1), Context 1, Stream 21, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/usecond       804.16
    SM Frequency            cycle/nsecond         1.99
    Elapsed Cycles                  cycle     17737642
    Memory Throughput                   %        79.95
    DRAM Throughput                     %        79.95
    Duration                      msecond         8.93
    L1/TEX Cache Throughput             %         1.70
    L2 Cache Throughput                 %         1.37
    SM Active Cycles                cycle     17574069
    Compute (SM) Throughput             %         2.59
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  16384
    Registers Per Thread             register/thread              70
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread          524288
    Waves Per SM                                               28.44
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           28
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        49.56
    Achieved Active Warps Per SM           warp        23.79
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

  _dptzyxcc2ccdptzyx(void *, void *, int) (32768, 1, 1)x(16, 1, 1), Context 1, Stream 25, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.01
    SM Frequency            cycle/nsecond         1.05
    Elapsed Cycles                  cycle      5378933
    Memory Throughput                   %        90.50
    DRAM Throughput                     %        90.50
    Duration                      msecond         5.14
    L1/TEX Cache Throughput             %        47.79
    L2 Cache Throughput                 %        39.25
    SM Active Cycles                cycle   5460393.21
    Compute (SM) Throughput             %         7.36
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    16
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  32768
    Registers Per Thread             register/thread              38
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread          524288
    Waves Per SM                                               56.89
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           48
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        49.67
    Achieved Active Warps Per SM           warp        23.84
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

  void copy_kernel<double>(cublasCopyParams<T1>) (196608, 1, 1)x(384, 1, 1), Context 1, Stream 25, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        88.15
    SM Frequency            cycle/usecond       357.53
    Elapsed Cycles                  cycle      1865868
    Memory Throughput                   %        47.42
    DRAM Throughput                     %         8.37
    Duration                      msecond         5.22
    L1/TEX Cache Throughput             %        94.84
    L2 Cache Throughput                 %         1.86
    SM Active Cycles                cycle   1845767.96
    Compute (SM) Throughput             %        51.38
    ----------------------- ------------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   384
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 196608
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread        75497472
    Waves Per SM                                                2048
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            5
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.69
    Achieved Active Warps Per SM           warp        38.25
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.31%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

  _tzyxsc2sctzyx(void *, void *, int) (32768, 1, 1)x(16, 1, 1), Context 1, Stream 25, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         7.91
    SM Frequency            cycle/nsecond         1.57
    Elapsed Cycles                  cycle      1225699
    Memory Throughput                   %        93.89
    DRAM Throughput                     %        93.89
    Duration                      usecond       778.69
    L1/TEX Cache Throughput             %        35.01
    L2 Cache Throughput                 %        42.15
    SM Active Cycles                cycle   1279997.04
    Compute (SM) Throughput             %         5.57
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    16
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  32768
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread          524288
    Waves Per SM                                               56.89
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block          128
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        49.17
    Achieved Active Warps Per SM           warp        23.60
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

  void copy_kernel<double>(cublasCopyParams<T1>) (32768, 1, 1)x(384, 1, 1), Context 1, Stream 25, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         7.89
    SM Frequency            cycle/nsecond         1.47
    Elapsed Cycles                  cycle      1146001
    Memory Throughput                   %        91.76
    DRAM Throughput                     %        91.76
    Duration                      usecond       781.57
    L1/TEX Cache Throughput             %        25.80
    L2 Cache Throughput                 %        26.44
    SM Active Cycles                cycle   1234223.62
    Compute (SM) Throughput             %        13.95
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   384
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  32768
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread        12582912
    Waves Per SM                                              341.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            5
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        67.21
    Achieved Active Warps Per SM           warp        32.26
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 32.79%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (67.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

  _tzyxsc2sctzyx(void *, void *, int) (32768, 1, 1)x(16, 1, 1), Context 1, Stream 25, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         7.96
    SM Frequency            cycle/nsecond         1.70
    Elapsed Cycles                  cycle      1324849
    Memory Throughput                   %        93.57
    DRAM Throughput                     %        93.57
    Duration                      usecond       777.70
    L1/TEX Cache Throughput             %        32.43
    L2 Cache Throughput                 %        35.09
    SM Active Cycles                cycle   1313408.17
    Compute (SM) Throughput             %         5.15
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    16
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  32768
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread          524288
    Waves Per SM                                               56.89
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block          128
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        49.19
    Achieved Active Warps Per SM           warp        23.61
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

  void copy_kernel<double>(cublasCopyParams<T1>) (32768, 1, 1)x(384, 1, 1), Context 1, Stream 25, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         7.94
    SM Frequency            cycle/nsecond         1.58
    Elapsed Cycles                  cycle      1232175
    Memory Throughput                   %        90.61
    DRAM Throughput                     %        90.61
    Duration                      usecond       780.26
    L1/TEX Cache Throughput             %        23.99
    L2 Cache Throughput                 %        23.46
    SM Active Cycles                cycle   1234761.92
    Compute (SM) Throughput             %        12.98
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   384
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  32768
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread        12582912
    Waves Per SM                                              341.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            5
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        66.50
    Achieved Active Warps Per SM           warp        31.92
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 33.5%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (66.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

  wilson_dslash_x_send(void *, void *, void *, int, void *, void *) (2048, 1, 1)x(16, 1, 1), Context 1, Stream 27, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         7.92
    SM Frequency            cycle/nsecond         1.78
    Elapsed Cycles                  cycle       384762
    Memory Throughput                   %        73.49
    DRAM Throughput                     %        73.49
    Duration                      usecond       215.62
    L1/TEX Cache Throughput             %        18.72
    L2 Cache Throughput                 %        29.96
    SM Active Cycles                cycle    354665.62
    Compute (SM) Throughput             %        66.00
    ----------------------- ------------- ------------

    INF   Compute and Memory are well-balanced: To reduce runtime, both computation and memory traffic must be reduced. 
          Check both the Compute Workload Analysis and Memory Workload Analysis sections.                               

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    16
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   2048
    Registers Per Thread             register/thread              86
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread           32768
    Waves Per SM                                                4.27
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           20
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           20
    Theoretical Occupancy                     %        41.67
    Achieved Occupancy                        %        37.12
    Achieved Active Warps Per SM           warp        17.82
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 58.33%                                                                                    
          The 5.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (41.7%) is limited by the number of required      
          registers.                                                                                                    

  wilson_dslash_y_send(void *, void *, void *, int, void *, void *) (1024, 1, 1)x(16, 1, 1), Context 1, Stream 29, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         6.75
    SM Frequency            cycle/nsecond         1.47
    Elapsed Cycles                  cycle       142652
    Memory Throughput                   %        37.50
    DRAM Throughput                     %        37.50
    Duration                      usecond        97.02
    L1/TEX Cache Throughput             %         7.69
    L2 Cache Throughput                 %        11.08
    SM Active Cycles                cycle    135170.50
    Compute (SM) Throughput             %        77.53
    ----------------------- ------------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    16
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              88
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread           16384
    Waves Per SM                                                2.13
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           20
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           20
    Theoretical Occupancy                     %        41.67
    Achieved Occupancy                        %        34.30
    Achieved Active Warps Per SM           warp        16.46
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 58.33%                                                                                    
          The 5.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (41.7%) is limited by the number of required      
          registers.                                                                                                    

  wilson_dslash_z_send(void *, void *, void *, int, void *, void *) (1024, 1, 1)x(16, 1, 1), Context 1, Stream 31, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/usecond       792.42
    SM Frequency            cycle/nsecond         2.25
    Elapsed Cycles                  cycle       867678
    Memory Throughput                   %        80.60
    DRAM Throughput                     %        80.60
    Duration                      usecond       385.09
    L1/TEX Cache Throughput             %         1.29
    L2 Cache Throughput                 %         1.98
    SM Active Cycles                cycle    855815.71
    Compute (SM) Throughput             %        14.63
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    16
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              86
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread           16384
    Waves Per SM                                                2.13
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           20
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           20
    Theoretical Occupancy                     %        41.67
    Achieved Occupancy                        %        35.58
    Achieved Active Warps Per SM           warp        17.08
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 58.33%                                                                                    
          The 5.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (41.7%) is limited by the number of required      
          registers.                                                                                                    

  wilson_dslash_t_send(void *, void *, void *, int, void *, void *) (1024, 1, 1)x(16, 1, 1), Context 1, Stream 33, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/usecond       797.15
    SM Frequency            cycle/nsecond         2.26
    Elapsed Cycles                  cycle       872906
    Memory Throughput                   %        80.08
    DRAM Throughput                     %        80.08
    Duration                      usecond       385.70
    L1/TEX Cache Throughput             %         1.28
    L2 Cache Throughput                 %         1.97
    SM Active Cycles                cycle    856806.50
    Compute (SM) Throughput             %        12.67
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    16
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              86
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread           16384
    Waves Per SM                                                2.13
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           20
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           20
    Theoretical Occupancy                     %        41.67
    Achieved Occupancy                        %        35.69
    Achieved Active Warps Per SM           warp        17.13
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 58.33%                                                                                    
          The 5.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (41.7%) is limited by the number of required      
          registers.                                                                                                    

  wilson_dslash_inside(void *, void *, void *, void *, int) (32768, 1, 1)x(16, 1, 1), Context 1, Stream 25, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         7.53
    SM Frequency            cycle/nsecond         1.20
    Elapsed Cycles                  cycle     36721735
    Memory Throughput                   %         8.12
    DRAM Throughput                     %         8.12
    Duration                      msecond        30.48
    L1/TEX Cache Throughput             %         3.69
    L2 Cache Throughput                 %         3.07
    SM Active Cycles                cycle  36665949.33
    Compute (SM) Throughput             %        86.66
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    16
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  32768
    Registers Per Thread             register/thread             164
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread          524288
    Waves Per SM                                              113.78
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           12
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        21.05
    Achieved Active Warps Per SM           warp        10.10
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75%                                                                                       
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (25.0%) is limited by the number of required      
          registers.                                                                                                    

  wilson_dslash_x_recv(void *, void *, void *, int, void *, void *) (2048, 1, 1)x(16, 1, 1), Context 1, Stream 25, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.34
    SM Frequency            cycle/nsecond         1.58
    Elapsed Cycles                  cycle       430291
    Memory Throughput                   %        46.63
    DRAM Throughput                     %        46.63
    Duration                      usecond       272.26
    L1/TEX Cache Throughput             %        21.67
    L2 Cache Throughput                 %        16.32
    SM Active Cycles                cycle    423071.33
    Compute (SM) Throughput             %        81.87
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    16
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   2048
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread           32768
    Waves Per SM                                                3.56
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           24
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        45.30
    Achieved Active Warps Per SM           warp        21.74
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM. This kernel's theoretical occupancy (50.0%) is limited by the number of required           
          registers.                                                                                                    

  wilson_dslash_y_recv(void *, void *, void *, int, void *, void *) (1024, 1, 1)x(16, 1, 1), Context 1, Stream 25, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.17
    SM Frequency            cycle/nsecond         1.62
    Elapsed Cycles                  cycle       192277
    Memory Throughput                   %        35.60
    DRAM Throughput                     %        35.60
    Duration                      usecond       118.37
    L1/TEX Cache Throughput             %        10.40
    L2 Cache Throughput                 %        11.31
    SM Active Cycles                cycle    186046.29
    Compute (SM) Throughput             %        78.82
    ----------------------- ------------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    16
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread           16384
    Waves Per SM                                                1.78
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           24
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        41.37
    Achieved Active Warps Per SM           warp        19.86
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM. This kernel's theoretical occupancy (50.0%) is limited by the number of required           
          registers.                                                                                                    

  wilson_dslash_z_recv(void *, void *, void *, int, void *, void *) (1024, 1, 1)x(16, 1, 1), Context 1, Stream 25, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.07
    SM Frequency            cycle/nsecond         1.62
    Elapsed Cycles                  cycle       221660
    Memory Throughput                   %        31.16
    DRAM Throughput                     %        31.16
    Duration                      usecond       136.93
    L1/TEX Cache Throughput             %         9.02
    L2 Cache Throughput                 %         9.82
    SM Active Cycles                cycle    214890.71
    Compute (SM) Throughput             %        79.48
    ----------------------- ------------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    16
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread           16384
    Waves Per SM                                                1.78
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           24
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        41.11
    Achieved Active Warps Per SM           warp        19.73
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM. This kernel's theoretical occupancy (50.0%) is limited by the number of required           
          registers.                                                                                                    

  wilson_dslash_t_recv(void *, void *, void *, int, void *, void *) (1024, 1, 1)x(16, 1, 1), Context 1, Stream 25, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         7.14
    SM Frequency            cycle/nsecond         1.46
    Elapsed Cycles                  cycle       193665
    Memory Throughput                   %        36.33
    DRAM Throughput                     %        36.33
    Duration                      usecond       132.67
    L1/TEX Cache Throughput             %        10.32
    L2 Cache Throughput                 %        11.32
    SM Active Cycles                cycle    185735.42
    Compute (SM) Throughput             %        78.26
    ----------------------- ------------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    16
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread           16384
    Waves Per SM                                                1.78
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           24
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        41.40
    Achieved Active Warps Per SM           warp        19.87
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM. This kernel's theoretical occupancy (50.0%) is limited by the number of required           
          registers.                                                                                                    

  _ccdptzyx2dptzyxcc(void *, void *, int) (32768, 1, 1)x(16, 1, 1), Context 1, Stream 25, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        12.18
    SM Frequency            cycle/nsecond         1.30
    Elapsed Cycles                  cycle      6610771
    Memory Throughput                   %        60.56
    DRAM Throughput                     %        60.56
    Duration                      msecond         5.07
    L1/TEX Cache Throughput             %        98.18
    L2 Cache Throughput                 %        56.11
    SM Active Cycles                cycle   6603455.50
    Compute (SM) Throughput             %         9.02
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    16
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  32768
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread          524288
    Waves Per SM                                               56.89
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           48
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        49.80
    Achieved Active Warps Per SM           warp        23.90
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

  void copy_kernel<double>(cublasCopyParams<T1>) (196608, 1, 1)x(384, 1, 1), Context 1, Stream 25, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond       107.84
    SM Frequency            cycle/usecond       354.15
    Elapsed Cycles                  cycle      1845851
    Memory Throughput                   %        47.94
    DRAM Throughput                     %         6.64
    Duration                      msecond         5.21
    L1/TEX Cache Throughput             %        95.87
    L2 Cache Throughput                 %         1.75
    SM Active Cycles                cycle   1844602.96
    Compute (SM) Throughput             %        51.93
    ----------------------- ------------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   384
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 196608
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread        75497472
    Waves Per SM                                                2048
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            5
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.68
    Achieved Active Warps Per SM           warp        38.25
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.32%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

  _sctzyx2tzyxsc(void *, void *, int) (32768, 1, 1)x(16, 1, 1), Context 1, Stream 25, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.05
    SM Frequency            cycle/nsecond         1.65
    Elapsed Cycles                  cycle      1326132
    Memory Throughput                   %        89.94
    DRAM Throughput                     %        89.94
    Duration                      usecond       805.60
    L1/TEX Cache Throughput             %        81.87
    L2 Cache Throughput                 %        62.65
    SM Active Cycles                cycle      1484346
    Compute (SM) Throughput             %         6.96
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    16
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  32768
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread          524288
    Waves Per SM                                               56.89
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        49.26
    Achieved Active Warps Per SM           warp        23.65
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

  void copy_kernel<double>(cublasCopyParams<T1>) (32768, 1, 1)x(384, 1, 1), Context 1, Stream 25, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         7.91
    SM Frequency            cycle/nsecond         1.47
    Elapsed Cycles                  cycle      1150078
    Memory Throughput                   %        91.87
    DRAM Throughput                     %        91.87
    Duration                      usecond       782.40
    L1/TEX Cache Throughput             %        25.70
    L2 Cache Throughput                 %        31.83
    SM Active Cycles                cycle   1169553.88
    Compute (SM) Throughput             %        13.90
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   384
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  32768
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread        12582912
    Waves Per SM                                              341.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            5
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        66.29
    Achieved Active Warps Per SM           warp        31.82
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 33.71%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (66.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

  _sctzyx2tzyxsc(void *, void *, int) (32768, 1, 1)x(16, 1, 1), Context 1, Stream 25, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         8.04
    SM Frequency            cycle/nsecond         1.48
    Elapsed Cycles                  cycle      1206231
    Memory Throughput                   %        88.83
    DRAM Throughput                     %        88.83
    Duration                      usecond       815.87
    L1/TEX Cache Throughput             %        90.03
    L2 Cache Throughput                 %        66.31
    SM Active Cycles                cycle   1190858.12
    Compute (SM) Throughput             %         7.71
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    16
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  32768
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread          524288
    Waves Per SM                                               56.89
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        49.14
    Achieved Active Warps Per SM           warp        23.59
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

  void copy_kernel<double>(cublasCopyParams<T1>) (32768, 1, 1)x(384, 1, 1), Context 1, Stream 25, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         7.96
    SM Frequency            cycle/nsecond         1.42
    Elapsed Cycles                  cycle      1111726
    Memory Throughput                   %        91.71
    DRAM Throughput                     %        91.71
    Duration                      usecond       783.55
    L1/TEX Cache Throughput             %        26.59
    L2 Cache Throughput                 %        24.37
    SM Active Cycles                cycle   1268339.17
    Compute (SM) Throughput             %        14.38
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   384
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  32768
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread        12582912
    Waves Per SM                                              341.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            5
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        66.30
    Achieved Active Warps Per SM           warp        31.82
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 33.7%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (66.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

  _dptzyxcc2ccdptzyx(void *, void *, int) (32768, 1, 1)x(16, 1, 1), Context 1, Stream 37, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         7.95
    SM Frequency            cycle/nsecond         1.22
    Elapsed Cycles                  cycle      6220231
    Memory Throughput                   %        92.24
    DRAM Throughput                     %        92.24
    Duration                      msecond         5.09
    L1/TEX Cache Throughput             %        41.22
    L2 Cache Throughput                 %        36.92
    SM Active Cycles                cycle   5236712.58
    Compute (SM) Throughput             %         6.37
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    16
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  32768
    Registers Per Thread             register/thread              38
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread          524288
    Waves Per SM                                               56.89
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           48
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        49.68
    Achieved Active Warps Per SM           warp        23.84
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

  void copy_kernel<double>(cublasCopyParams<T1>) (196608, 1, 1)x(384, 1, 1), Context 1, Stream 37, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond       107.36
    SM Frequency            cycle/usecond       352.58
    Elapsed Cycles                  cycle      1845217
    Memory Throughput                   %        47.95
    DRAM Throughput                     %         6.87
    Duration                      msecond         5.23
    L1/TEX Cache Throughput             %        95.96
    L2 Cache Throughput                 %         1.88
    SM Active Cycles                cycle   1844051.46
    Compute (SM) Throughput             %        51.95
    ----------------------- ------------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   384
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 196608
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread        75497472
    Waves Per SM                                                2048
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            5
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.72
    Achieved Active Warps Per SM           warp        38.27
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.28%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

  _tzyxsc2sctzyx(void *, void *, int) (32768, 1, 1)x(16, 1, 1), Context 1, Stream 37, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         7.96
    SM Frequency            cycle/nsecond         1.60
    Elapsed Cycles                  cycle      1242077
    Memory Throughput                   %        93.74
    DRAM Throughput                     %        93.74
    Duration                      usecond       776.93
    L1/TEX Cache Throughput             %        34.57
    L2 Cache Throughput                 %        42.91
    SM Active Cycles                cycle   1386134.21
    Compute (SM) Throughput             %         5.50
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    16
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  32768
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread          524288
    Waves Per SM                                               56.89
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block          128
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        49.19
    Achieved Active Warps Per SM           warp        23.61
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

  void copy_kernel<double>(cublasCopyParams<T1>) (32768, 1, 1)x(384, 1, 1), Context 1, Stream 37, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         7.94
    SM Frequency            cycle/nsecond         1.37
    Elapsed Cycles                  cycle      1079391
    Memory Throughput                   %        90.80
    DRAM Throughput                     %        90.80
    Duration                      usecond       782.75
    L1/TEX Cache Throughput             %        27.54
    L2 Cache Throughput                 %        26.08
    SM Active Cycles                cycle   1296676.71
    Compute (SM) Throughput             %        14.88
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   384
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  32768
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread        12582912
    Waves Per SM                                              341.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            5
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        66.60
    Achieved Active Warps Per SM           warp        31.97
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 33.4%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (66.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

  _tzyxsc2sctzyx(void *, void *, int) (32768, 1, 1)x(16, 1, 1), Context 1, Stream 37, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         7.94
    SM Frequency            cycle/nsecond         1.68
    Elapsed Cycles                  cycle      1310165
    Memory Throughput                   %        93.71
    DRAM Throughput                     %        93.71
    Duration                      usecond       778.59
    L1/TEX Cache Throughput             %        32.78
    L2 Cache Throughput                 %        36.31
    SM Active Cycles                cycle   1300866.83
    Compute (SM) Throughput             %         5.21
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    16
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  32768
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread          524288
    Waves Per SM                                               56.89
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block          128
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        49.24
    Achieved Active Warps Per SM           warp        23.63
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

  void copy_kernel<double>(cublasCopyParams<T1>) (32768, 1, 1)x(384, 1, 1), Context 1, Stream 37, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         7.91
    SM Frequency            cycle/nsecond         1.63
    Elapsed Cycles                  cycle      1276638
    Memory Throughput                   %        92.32
    DRAM Throughput                     %        92.32
    Duration                      usecond       781.89
    L1/TEX Cache Throughput             %        23.17
    L2 Cache Throughput                 %        19.86
    SM Active Cycles                cycle   1208443.79
    Compute (SM) Throughput             %        12.52
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   384
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  32768
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread        12582912
    Waves Per SM                                              341.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            5
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        66.65
    Achieved Active Warps Per SM           warp        31.99
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 33.35%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (66.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

  wilson_dslash_x_send(void *, void *, void *, int, void *, void *) (2048, 1, 1)x(16, 1, 1), Context 1, Stream 39, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         7.66
    SM Frequency            cycle/nsecond         1.74
    Elapsed Cycles                  cycle       386228
    Memory Throughput                   %        73.95
    DRAM Throughput                     %        73.95
    Duration                      usecond       222.05
    L1/TEX Cache Throughput             %        18.78
    L2 Cache Throughput                 %        28.70
    SM Active Cycles                cycle    375710.46
    Compute (SM) Throughput             %        65.75
    ----------------------- ------------- ------------

    INF   Compute and Memory are well-balanced: To reduce runtime, both computation and memory traffic must be reduced. 
          Check both the Compute Workload Analysis and Memory Workload Analysis sections.                               

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    16
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   2048
    Registers Per Thread             register/thread              86
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread           32768
    Waves Per SM                                                4.27
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           20
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           20
    Theoretical Occupancy                     %        41.67
    Achieved Occupancy                        %        37.09
    Achieved Active Warps Per SM           warp        17.80
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 58.33%                                                                                    
          The 5.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (41.7%) is limited by the number of required      
          registers.                                                                                                    

  wilson_dslash_y_send(void *, void *, void *, int, void *, void *) (1024, 1, 1)x(16, 1, 1), Context 1, Stream 41, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         7.28
    SM Frequency            cycle/nsecond         1.68
    Elapsed Cycles                  cycle       143809
    Memory Throughput                   %        39.46
    DRAM Throughput                     %        39.46
    Duration                      usecond        85.50
    L1/TEX Cache Throughput             %         7.62
    L2 Cache Throughput                 %        11.16
    SM Active Cycles                cycle    136463.17
    Compute (SM) Throughput             %        76.90
    ----------------------- ------------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    16
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              88
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread           16384
    Waves Per SM                                                2.13
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           20
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           20
    Theoretical Occupancy                     %        41.67
    Achieved Occupancy                        %        34.30
    Achieved Active Warps Per SM           warp        16.46
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 58.33%                                                                                    
          The 5.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (41.7%) is limited by the number of required      
          registers.                                                                                                    

  wilson_dslash_z_send(void *, void *, void *, int, void *, void *) (1024, 1, 1)x(16, 1, 1), Context 1, Stream 43, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         6.52
    SM Frequency            cycle/nsecond         1.41
    Elapsed Cycles                  cycle       158386
    Memory Throughput                   %        33.50
    DRAM Throughput                     %        33.50
    Duration                      usecond       112.48
    L1/TEX Cache Throughput             %         6.89
    L2 Cache Throughput                 %         9.98
    SM Active Cycles                cycle    151755.04
    Compute (SM) Throughput             %        80.17
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    16
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              86
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread           16384
    Waves Per SM                                                2.13
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           20
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           20
    Theoretical Occupancy                     %        41.67
    Achieved Occupancy                        %        34.55
    Achieved Active Warps Per SM           warp        16.59
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 58.33%                                                                                    
          The 5.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (41.7%) is limited by the number of required      
          registers.                                                                                                    

  wilson_dslash_t_send(void *, void *, void *, int, void *, void *) (1024, 1, 1)x(16, 1, 1), Context 1, Stream 45, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         7.40
    SM Frequency            cycle/nsecond         1.69
    Elapsed Cycles                  cycle       143580
    Memory Throughput                   %        39.20
    DRAM Throughput                     %        39.20
    Duration                      usecond        84.74
    L1/TEX Cache Throughput             %         7.61
    L2 Cache Throughput                 %        11.09
    SM Active Cycles                cycle    136529.08
    Compute (SM) Throughput             %        77.03
    ----------------------- ------------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    16
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              86
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread           16384
    Waves Per SM                                                2.13
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           20
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           20
    Theoretical Occupancy                     %        41.67
    Achieved Occupancy                        %        34.50
    Achieved Active Warps Per SM           warp        16.56
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 58.33%                                                                                    
          The 5.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (41.7%) is limited by the number of required      
          registers.                                                                                                    

  wilson_dslash_inside(void *, void *, void *, void *, int) (32768, 1, 1)x(16, 1, 1), Context 1, Stream 37, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        11.19
    SM Frequency            cycle/nsecond         1.68
    Elapsed Cycles                  cycle     36736357
    Memory Throughput                   %         7.64
    DRAM Throughput                     %         7.64
    Duration                      msecond        21.83
    L1/TEX Cache Throughput             %         3.64
    L2 Cache Throughput                 %         2.98
    SM Active Cycles                cycle  36676686.54
    Compute (SM) Throughput             %        86.62
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    16
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  32768
    Registers Per Thread             register/thread             164
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread          524288
    Waves Per SM                                              113.78
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           12
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        22.18
    Achieved Active Warps Per SM           warp        10.64
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75%                                                                                       
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (25.0%) is limited by the number of required      
          registers.                                                                                                    

  wilson_dslash_x_recv(void *, void *, void *, int, void *, void *) (2048, 1, 1)x(16, 1, 1), Context 1, Stream 37, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/usecond       783.93
    SM Frequency            cycle/nsecond         2.07
    Elapsed Cycles                  cycle      3541538
    Memory Throughput                   %        78.92
    DRAM Throughput                     %        78.92
    Duration                      msecond         1.71
    L1/TEX Cache Throughput             %         2.63
    L2 Cache Throughput                 %         3.50
    SM Active Cycles                cycle   3494232.62
    Compute (SM) Throughput             %         9.95
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    16
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   2048
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread           32768
    Waves Per SM                                                3.56
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           24
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        45.99
    Achieved Active Warps Per SM           warp        22.07
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM. This kernel's theoretical occupancy (50.0%) is limited by the number of required           
          registers.                                                                                                    

  wilson_dslash_y_recv(void *, void *, void *, int, void *, void *) (1024, 1, 1)x(16, 1, 1), Context 1, Stream 37, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         1.44
    SM Frequency            cycle/usecond       370.14
    Elapsed Cycles                  cycle       195615
    Memory Throughput                   %        45.21
    DRAM Throughput                     %        45.21
    Duration                      usecond       528.48
    L1/TEX Cache Throughput             %        10.22
    L2 Cache Throughput                 %        11.88
    SM Active Cycles                cycle    189155.38
    Compute (SM) Throughput             %        77.48
    ----------------------- ------------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    16
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread           16384
    Waves Per SM                                                1.78
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           24
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        41.33
    Achieved Active Warps Per SM           warp        19.84
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM. This kernel's theoretical occupancy (50.0%) is limited by the number of required           
          registers.                                                                                                    

  wilson_dslash_z_recv(void *, void *, void *, int, void *, void *) (1024, 1, 1)x(16, 1, 1), Context 1, Stream 37, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         7.82
    SM Frequency            cycle/nsecond         1.56
    Elapsed Cycles                  cycle       221748
    Memory Throughput                   %        30.87
    DRAM Throughput                     %        30.87
    Duration                      usecond       142.46
    L1/TEX Cache Throughput             %         9.01
    L2 Cache Throughput                 %         9.81
    SM Active Cycles                cycle    215145.58
    Compute (SM) Throughput             %        79.43
    ----------------------- ------------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    16
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread           16384
    Waves Per SM                                                1.78
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           24
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        41.10
    Achieved Active Warps Per SM           warp        19.73
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM. This kernel's theoretical occupancy (50.0%) is limited by the number of required           
          registers.                                                                                                    

  wilson_dslash_t_recv(void *, void *, void *, int, void *, void *) (1024, 1, 1)x(16, 1, 1), Context 1, Stream 37, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         7.57
    SM Frequency            cycle/nsecond         1.56
    Elapsed Cycles                  cycle       194035
    Memory Throughput                   %        36.62
    DRAM Throughput                     %        36.62
    Duration                      usecond       124.19
    L1/TEX Cache Throughput             %        10.30
    L2 Cache Throughput                 %        11.20
    SM Active Cycles                cycle    186721.75
    Compute (SM) Throughput             %        78.11
    ----------------------- ------------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    16
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread           16384
    Waves Per SM                                                1.78
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           24
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        41.29
    Achieved Active Warps Per SM           warp        19.82
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM. This kernel's theoretical occupancy (50.0%) is limited by the number of required           
          registers.                                                                                                    

  _ccdptzyx2dptzyxcc(void *, void *, int) (32768, 1, 1)x(16, 1, 1), Context 1, Stream 37, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.90
    SM Frequency            cycle/nsecond         1.30
    Elapsed Cycles                  cycle      6611697
    Memory Throughput                   %        74.51
    DRAM Throughput                     %        74.51
    Duration                      msecond         5.08
    L1/TEX Cache Throughput             %        98.17
    L2 Cache Throughput                 %        69.35
    SM Active Cycles                cycle   6603478.67
    Compute (SM) Throughput             %         9.15
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    16
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  32768
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread          524288
    Waves Per SM                                               56.89
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           48
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        49.80
    Achieved Active Warps Per SM           warp        23.90
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

  void copy_kernel<double>(cublasCopyParams<T1>) (196608, 1, 1)x(384, 1, 1), Context 1, Stream 37, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond       107.44
    SM Frequency            cycle/usecond       352.83
    Elapsed Cycles                  cycle      1844341
    Memory Throughput                   %        47.97
    DRAM Throughput                     %         6.87
    Duration                      msecond         5.23
    L1/TEX Cache Throughput             %        95.96
    L2 Cache Throughput                 %         1.89
    SM Active Cycles                cycle   1843402.25
    Compute (SM) Throughput             %        51.98
    ----------------------- ------------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   384
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 196608
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread        75497472
    Waves Per SM                                                2048
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            5
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.70
    Achieved Active Warps Per SM           warp        38.26
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.3%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

  _sctzyx2tzyxsc(void *, void *, int) (32768, 1, 1)x(16, 1, 1), Context 1, Stream 37, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/usecond       699.92
    SM Frequency            cycle/usecond       140.47
    Elapsed Cycles                  cycle      1302378
    Memory Throughput                   %        89.89
    DRAM Throughput                     %        89.89
    Duration                      msecond         9.27
    L1/TEX Cache Throughput             %        83.37
    L2 Cache Throughput                 %        63.37
    SM Active Cycles                cycle   1612594.54
    Compute (SM) Throughput             %         6.82
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    16
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  32768
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread          524288
    Waves Per SM                                               56.89
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        49.25
    Achieved Active Warps Per SM           warp        23.64
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

  void copy_kernel<double>(cublasCopyParams<T1>) (32768, 1, 1)x(384, 1, 1), Context 1, Stream 37, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         7.94
    SM Frequency            cycle/nsecond         1.53
    Elapsed Cycles                  cycle      1205420
    Memory Throughput                   %        90.56
    DRAM Throughput                     %        90.56
    Duration                      usecond       785.66
    L1/TEX Cache Throughput             %        24.53
    L2 Cache Throughput                 %        26.00
    SM Active Cycles                cycle   1194642.58
    Compute (SM) Throughput             %        13.26
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   384
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  32768
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread        12582912
    Waves Per SM                                              341.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            5
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        66.37
    Achieved Active Warps Per SM           warp        31.86
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 33.63%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (66.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

  _sctzyx2tzyxsc(void *, void *, int) (32768, 1, 1)x(16, 1, 1), Context 1, Stream 37, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         7.77
    SM Frequency            cycle/nsecond         1.41
    Elapsed Cycles                  cycle      1196666
    Memory Throughput                   %        88.69
    DRAM Throughput                     %        88.69
    Duration                      usecond       846.24
    L1/TEX Cache Throughput             %        90.74
    L2 Cache Throughput                 %        67.53
    SM Active Cycles                cycle   1301113.17
    Compute (SM) Throughput             %         7.92
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    16
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  32768
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread          524288
    Waves Per SM                                               56.89
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        49.21
    Achieved Active Warps Per SM           warp        23.62
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

  void copy_kernel<double>(cublasCopyParams<T1>) (32768, 1, 1)x(384, 1, 1), Context 1, Stream 37, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         7.96
    SM Frequency            cycle/nsecond         1.46
    Elapsed Cycles                  cycle      1143537
    Memory Throughput                   %        92.01
    DRAM Throughput                     %        92.01
    Duration                      usecond       781.60
    L1/TEX Cache Throughput             %        25.88
    L2 Cache Throughput                 %        22.77
    SM Active Cycles                cycle   1148216.08
    Compute (SM) Throughput             %        13.99
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   384
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  32768
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread        12582912
    Waves Per SM                                              341.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            5
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        66.55
    Achieved Active Warps Per SM           warp        31.95
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 33.45%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (66.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

  cupy_subtract__complex128_complex128_complex128 (98304, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/usecond       806.76
    SM Frequency            cycle/nsecond         1.82
    Elapsed Cycles                  cycle     52858058
    Memory Throughput                   %        79.08
    DRAM Throughput                     %        79.08
    Duration                      msecond        29.07
    L1/TEX Cache Throughput             %         1.30
    L2 Cache Throughput                 %         1.56
    SM Active Cycles                cycle  53572301.38
    Compute (SM) Throughput             %         0.99
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  98304
    Registers Per Thread             register/thread              26
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread        12582912
    Waves Per SM                                              341.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        91.01
    Achieved Active Warps Per SM           warp        43.69
    ------------------------------- ----------- ------------

  cupy_absolute__complex128_float64 (98304, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/usecond       806.29
    SM Frequency            cycle/nsecond         1.81
    Elapsed Cycles                  cycle     25815012
    Memory Throughput                   %        79.22
    DRAM Throughput                     %        79.22
    Duration                      msecond        14.25
    L1/TEX Cache Throughput             %         1.27
    L2 Cache Throughput                 %         1.59
    SM Active Cycles                cycle  26271126.50
    Compute (SM) Throughput             %        15.23
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  98304
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread        12582912
    Waves Per SM                                              341.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        88.06
    Achieved Active Warps Per SM           warp        42.27
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 11.94%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (88.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

  cupy_multiply__float64_float64_float64 (98304, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/usecond       804.38
    SM Frequency            cycle/nsecond         1.88
    Elapsed Cycles                  cycle     16923902
    Memory Throughput                   %        79.89
    DRAM Throughput                     %        79.89
    Duration                      msecond         8.99
    L1/TEX Cache Throughput             %         1.74
    L2 Cache Throughput                 %         1.40
    SM Active Cycles                cycle  16888050.96
    Compute (SM) Throughput             %         1.55
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  98304
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread        12582912
    Waves Per SM                                              341.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           21
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.34
    Achieved Active Warps Per SM           warp        46.25
    ------------------------------- ----------- ------------

  void cub::CUB_200101_500_520_600_610_700_750_800_860_890_900_NS::DeviceReduceKernel<cub::CUB_200101_500_520_600_610_700_750_800_860_890_900_NS::DeviceReducePolicy<double, unsigned int, cuda::std::__4::plus<void>>::Policy600, double *, unsigned int, cuda::std::__4::plus<void>, double>(T2, T5 *, T3, cub::CUB_200101_500_520_600_610_700_750_800_860_890_900_NS::GridEvenShare<T3>, T4) (720, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/usecond       806.47
    SM Frequency            cycle/nsecond         2.15
    Elapsed Cycles                  cycle     10295102
    Memory Throughput                   %        81.82
    DRAM Throughput                     %        81.82
    Duration                      msecond         4.78
    L1/TEX Cache Throughput             %         1.50
    L2 Cache Throughput                 %         2.10
    SM Active Cycles                cycle  10338948.46
    Compute (SM) Throughput             %         2.73
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    720
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block              80
    Threads                                   thread          184320
    Waves Per SM                                                   5
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           88
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        93.80
    Achieved Active Warps Per SM           warp        45.02
    ------------------------------- ----------- ------------

  void cub::CUB_200101_500_520_600_610_700_750_800_860_890_900_NS::DeviceReduceSingleTileKernel<cub::CUB_200101_500_520_600_610_700_750_800_860_890_900_NS::DeviceReducePolicy<double, unsigned int, cuda::std::__4::plus<void>>::Policy600, double *, double *, int, cuda::std::__4::plus<void>, double, double>(T2, T3, T4, T5, T6) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         7.49
    SM Frequency            cycle/nsecond         1.89
    Elapsed Cycles                  cycle         5677
    Memory Throughput                   %         1.21
    DRAM Throughput                     %         0.82
    Duration                      usecond         3.01
    L1/TEX Cache Throughput             %         7.37
    L2 Cache Throughput                 %         1.21
    SM Active Cycles                cycle       149.33
    Compute (SM) Throughput             %         0.74
    ----------------------- ------------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              44
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block              80
    Threads                                   thread             256
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 95.83%                                                                                          
          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 24              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            5
    Block Limit Shared Mem                block           88
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           40
    Theoretical Occupancy                     %        83.33
    Achieved Occupancy                        %        14.98
    Achieved Active Warps Per SM           warp         7.19
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 82.02%                                                                                    
          The difference between calculated theoretical (83.3%) and measured achieved occupancy (15.0%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

  cupy_sqrt__float64_float64 (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/usecond       666.67
    SM Frequency            cycle/nsecond         1.76
    Elapsed Cycles                  cycle         7427
    Memory Throughput                   %         0.33
    DRAM Throughput                     %         0.14
    Duration                      usecond         4.22
    L1/TEX Cache Throughput             %         4.12
    L2 Cache Throughput                 %         0.33
    SM Active Cycles                cycle       218.58
    Compute (SM) Throughput             %         0.07
    ----------------------- ------------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              20
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread               1
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 96.88%                                                                                          
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 95.83%                                                                                          
          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 24              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %         2.08
    Achieved Active Warps Per SM           warp            1
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 95.83%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (2.1%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

  cupy_absolute__complex128_float64 (98304, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.38
    SM Frequency            cycle/nsecond         1.92
    Elapsed Cycles                  cycle      4625615
    Memory Throughput                   %        40.19
    DRAM Throughput                     %        40.19
    Duration                      msecond         2.41
    L1/TEX Cache Throughput             %         7.09
    L2 Cache Throughput                 %         8.58
    SM Active Cycles                cycle   4623153.25
    Compute (SM) Throughput             %        85.01
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  98304
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread        12582912
    Waves Per SM                                              341.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        93.93
    Achieved Active Warps Per SM           warp        45.09
    ------------------------------- ----------- ------------

  cupy_multiply__float64_float64_float64 (98304, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/usecond       803.53
    SM Frequency            cycle/nsecond         1.83
    Elapsed Cycles                  cycle     16523463
    Memory Throughput                   %        79.87
    DRAM Throughput                     %        79.87
    Duration                      msecond         9.01
    L1/TEX Cache Throughput             %         1.78
    L2 Cache Throughput                 %         1.42
    SM Active Cycles                cycle  16363432.67
    Compute (SM) Throughput             %         1.59
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  98304
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread        12582912
    Waves Per SM                                              341.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           21
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        92.87
    Achieved Active Warps Per SM           warp        44.58
    ------------------------------- ----------- ------------

  void cub::CUB_200101_500_520_600_610_700_750_800_860_890_900_NS::DeviceReduceKernel<cub::CUB_200101_500_520_600_610_700_750_800_860_890_900_NS::DeviceReducePolicy<double, unsigned int, cuda::std::__4::plus<void>>::Policy600, double *, unsigned int, cuda::std::__4::plus<void>, double>(T2, T5 *, T3, cub::CUB_200101_500_520_600_610_700_750_800_860_890_900_NS::GridEvenShare<T3>, T4) (720, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/usecond       797.50
    SM Frequency            cycle/nsecond         2.00
    Elapsed Cycles                  cycle      9654482
    Memory Throughput                   %        81.83
    DRAM Throughput                     %        81.83
    Duration                      msecond         4.83
    L1/TEX Cache Throughput             %         1.41
    L2 Cache Throughput                 %         2.20
    SM Active Cycles                cycle  10924015.08
    Compute (SM) Throughput             %         2.91
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    720
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block              80
    Threads                                   thread          184320
    Waves Per SM                                                   5
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           88
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        93.79
    Achieved Active Warps Per SM           warp        45.02
    ------------------------------- ----------- ------------

  void cub::CUB_200101_500_520_600_610_700_750_800_860_890_900_NS::DeviceReduceSingleTileKernel<cub::CUB_200101_500_520_600_610_700_750_800_860_890_900_NS::DeviceReducePolicy<double, unsigned int, cuda::std::__4::plus<void>>::Policy600, double *, double *, int, cuda::std::__4::plus<void>, double, double>(T2, T3, T4, T5, T6) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/usecond       673.68
    SM Frequency            cycle/nsecond         1.94
    Elapsed Cycles                  cycle        11785
    Memory Throughput                   %         4.49
    DRAM Throughput                     %         4.49
    Duration                      usecond         6.08
    L1/TEX Cache Throughput             %         2.74
    L2 Cache Throughput                 %         0.61
    SM Active Cycles                cycle       401.54
    Compute (SM) Throughput             %         0.36
    ----------------------- ------------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              44
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block              80
    Threads                                   thread             256
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 95.83%                                                                                          
          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 24              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            5
    Block Limit Shared Mem                block           88
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           40
    Theoretical Occupancy                     %        83.33
    Achieved Occupancy                        %        16.04
    Achieved Active Warps Per SM           warp         7.70
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 80.75%                                                                                    
          The difference between calculated theoretical (83.3%) and measured achieved occupancy (16.0%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

  cupy_sqrt__float64_float64 (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         6.68
    SM Frequency            cycle/nsecond         1.67
    Elapsed Cycles                  cycle         3896
    Memory Throughput                   %         0.49
    DRAM Throughput                     %         0.05
    Duration                      usecond         2.34
    L1/TEX Cache Throughput             %        11.04
    L2 Cache Throughput                 %         0.49
    SM Active Cycles                cycle        81.50
    Compute (SM) Throughput             %         0.14
    ----------------------- ------------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              20
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread               1
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 96.88%                                                                                          
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 95.83%                                                                                          
          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 24              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           84
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %         2.08
    Achieved Active Warps Per SM           warp            1
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 95.83%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (2.1%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

  cupy_true_divide__float64_float64_float64 (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/usecond       547.95
    SM Frequency            cycle/nsecond         1.69
    Elapsed Cycles                  cycle         7895
    Memory Throughput                   %         0.32
    DRAM Throughput                     %         0.31
    Duration                      usecond         4.67
    L1/TEX Cache Throughput             %         3.55
    L2 Cache Throughput                 %         0.32
    SM Active Cycles                cycle       253.83
    Compute (SM) Throughput             %         0.07
    ----------------------- ------------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              26
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread               1
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 96.88%                                                                                          
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 95.83%                                                                                          
          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 24              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           64
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %         2.08
    Achieved Active Warps Per SM           warp            1
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 95.83%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (2.1%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            


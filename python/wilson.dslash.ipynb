{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyquda.utils import gauge_utils\n",
    "from pyquda.field import LatticeFermion\n",
    "from pyquda.enum_quda import QudaParity\n",
    "import pyquda.pyqcu as pyquda_pyqcu\n",
    "from pyquda import init, core, quda, mpi\n",
    "import os\n",
    "import sys\n",
    "from time import perf_counter\n",
    "import cupy as cp\n",
    "__file__ = \".\"\n",
    "test_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "sys.path.insert(0, os.path.join(test_dir, \"..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disabling GPU-Direct RDMA access\n",
      "Enabling peer-to-peer copy engine and direct load/store access\n",
      "QUDA 1.1.0 (git 1.1.0--sm_80)\n",
      "CUDA Driver version = 12040\n",
      "CUDA Runtime version = 12030\n",
      "Found device 0: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "WARNING: ** Running on a device with compute capability 8.9 but QUDA was compiled for 8.0. **\n",
      " -- This might result in a lower performance. Please consider adjusting QUDA_GPU_ARCH when running cmake.\n",
      "\n",
      "Using device 0: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "WARNING: Data reordering done on GPU (set with QUDA_REORDER_LOCATION=GPU/CPU)\n",
      "Loaded 20 sets of cached parameters from .cache/tunecache.tsv\n",
      "Loaded 20 sets of cached parameters from .cache/tunecache.tsv\n",
      "WARNING: Using device memory pool allocator\n",
      "WARNING: Using pinned memory pool allocator\n",
      "cublasCreated successfully\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"QUDA_RESOURCE_PATH\"] = \".cache\"\n",
    "latt_size = [32, 32, 32, 64]\n",
    "grid_size = [1, 1, 1, 1]\n",
    "Lx, Ly, Lz, Lt = latt_size\n",
    "Nd, Ns, Nc = 4, 4, 3\n",
    "Gx, Gy, Gz, Gt = grid_size\n",
    "latt_size = [Lx // Gx, Ly // Gy, Lz // Gz, Lt // Gt]\n",
    "Lx, Ly, Lz, Lt = latt_size\n",
    "Vol = Lx * Ly * Lz * Lt\n",
    "mpi.init(grid_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============round  1 ======================\n",
      "Creating Gaussian distrbuted Lie group field with sigma = 1.000000e-01\n",
      "Quda dslash: 0.18595754199486692 sec\n"
     ]
    }
   ],
   "source": [
    "round = 1\n",
    "# generate a vector p randomly\n",
    "p = LatticeFermion(latt_size, cp.random.randn(\n",
    "    Lt, Lz, Ly, Lx, Ns, Nc * 2).view(cp.complex128))\n",
    "Mp = LatticeFermion(latt_size)\n",
    "# Mp2 = LatticeFermion(latt_size)\n",
    "print('===============round ', round, '======================')\n",
    "# Set parameters in Dslash and use m=-3.5 to make kappa=1\n",
    "dslash = core.getDslash(latt_size, -3.5, 0, 0, anti_periodic_t=False)\n",
    "# Generate gauge and then load it\n",
    "U = gauge_utils.gaussGauge(latt_size, round)\n",
    "dslash.loadGauge(U)\n",
    "cp.cuda.runtime.deviceSynchronize()\n",
    "t1 = perf_counter()\n",
    "quda.dslashQuda(Mp.even_ptr, p.odd_ptr, dslash.invert_param,\n",
    "                QudaParity.QUDA_EVEN_PARITY)\n",
    "quda.dslashQuda(Mp.odd_ptr, p.even_ptr, dslash.invert_param,\n",
    "                QudaParity.QUDA_ODD_PARITY)\n",
    "cp.cuda.runtime.deviceSynchronize()\n",
    "t2 = perf_counter()\n",
    "print(f'Quda dslash: {t2 - t1} sec')\n",
    "# # then execute my code\n",
    "# param = pyquda_pyqcu.QcuParam()\n",
    "# param.lattice_size = latt_size\n",
    "# grid = pyquda_pyqcu.QcuParam()\n",
    "# grid.lattice_size = grid_size\n",
    "# cp.cuda.runtime.deviceSynchronize()\n",
    "# t1 = perf_counter()\n",
    "# pyquda_pyqcu.mpiDslashQcu(Mp1.even_ptr, p.odd_ptr, U.data_ptr, param, 0, grid)\n",
    "# pyquda_pyqcu.mpiDslashQcu(Mp1.odd_ptr, p.even_ptr, U.data_ptr, param, 1, grid)\n",
    "# cp.cuda.runtime.deviceSynchronize()\n",
    "# t2 = perf_counter()\n",
    "# print(f'QCU dslash: {t2 - t1} sec')\n",
    "# print('quda difference: ', cp.linalg.norm(Mp1.data - Mp.data) / cp.linalg.norm(Mp.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'cupy.ndarray'>\n",
      "2097152\n",
      "(2, 64, 32, 32, 16, 4, 3)\n",
      "(1, 64, 32, 32, 16, 4, 3)\n",
      "32\n",
      "[[0 1 2]\n",
      " [3 4 5]\n",
      " [6 7 8]]\n",
      "[[  0   2   8]\n",
      " [ 18  32  50]\n",
      " [ 72  98 128]]\n",
      "(2, 64, 32, 32, 16, 4, 3)\n",
      "(64, 32, 32, 16, 4)\n",
      "(4, 16, 32, 32, 64)\n"
     ]
    }
   ],
   "source": [
    "print(type(Mp.data))\n",
    "print(Lx*Ly*Lz*Lt)\n",
    "print(Mp.data.shape)\n",
    "print(Mp.data[len(Mp.data)/2:].shape)\n",
    "print(Mp.data[0].shape[2])\n",
    "x_gpu = cp.array(range(9)).reshape([3,3])\n",
    "print(x_gpu)\n",
    "print(x_gpu*x_gpu*2)\n",
    "print(Mp.data.shape)\n",
    "print(Mp.data.shape[1:-1])\n",
    "print(Mp.data.shape[1:-1][::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 32, 32, 64)\n",
      "(16, 32, 32, 64)\n"
     ]
    }
   ],
   "source": [
    "import pyqcu.wilson_dslash\n",
    "pyqcu_Mp = LatticeFermion(latt_size)\n",
    "pyqcu.wilson_dslash.run(src=p.data, dest=pyqcu_Mp.data, U=U.data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
